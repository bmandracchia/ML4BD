
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Probabilistic Learning &#8212; Introduction to Machine Learning for Biomedical Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/28_probabilistic';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Physics-Based Deep Learning" href="29_PBDL.html" />
    <link rel="prev" title="Attention Mechanisms and Transformers" href="27_transformers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Machine Learning for Biomedical Data - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Machine Learning for Biomedical Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Computational Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_python_primer.html">Python: a Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_data_science_with_pandas.html">Data Science with Python and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_jupyter_markdown.html">Jupyter &amp; Markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Feature Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_machine_learning_fundamentals.html">The Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_basic_feature_engineering.html">Basic Feature Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_categorical_variables.html">Categorical Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_classifiers.html">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_dimensionality_reduction.html">Dimensionality Reduction: PCA</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_kmeans.html">Nonlinear Manifold Feature Extraction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="21_neural%20networks.html">Deep Learning in Biomedical Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="22_NN_as_UA.html">Neural Networks as Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_Loss_Metrics_and_Optimizers.html">Training a Digit Classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="24_MLP.html">Classification with Multilayer Perceptrons</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_convolutions.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="26_resnet2.html">ResNets for Biomedical Image Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="27_transformers.html">Attention Mechanisms and Transformers</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Probabilistic Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_PBDL.html">Physics-Based Deep Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD/issues/new?title=Issue%20on%20page%20%2Fchapters/28_probabilistic.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/28_probabilistic.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probabilistic Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty">Uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-or-backward">Forward or Backward?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-based-inference">Simulation-Based Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-deep-learning">Leveraging Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-a-probability-distribution">Learning a Probability Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-a-training-objective">Fundamentals: A Training Objective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks">Generative Adversarial Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adversarial-training">Adversarial Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gans">Conditional GANs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ambiguous-solutions">Ambiguous Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spatio-temporal-super-resolution">Spatio-Temporal Super-Resolution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-distributions-with-normalizing-flows">Learning Distributions with Normalizing Flows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-mappings">Invertible mappings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-density-via-change-of-variables">Exact density via change of variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-densities-and-volume-correction">Transforming densities and volume correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flows-for-images">Flows for images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-inference">Training, validation, and inference</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probabilistic-learning">
<h1>Probabilistic Learning<a class="headerlink" href="#probabilistic-learning" title="Link to this heading">#</a></h1>
<p>Up to this point we have treated the target mapping as <strong>deterministic</strong>: for every input <span class="math notranslate nohighlight">\(x\)</span> there is a single, correct output <span class="math notranslate nohighlight">\(y=f(x)\)</span>. That assumption is a drastic simplification. In realistic settings, solutions may be <strong>ambiguous</strong>, our learned model may <strong>confuse</strong> similar cases, and these effects often <strong>interact</strong>. This motivates a <strong>probabilistic</strong> treatment. Conceptually, we add a new axis to the problem: instead of a single outcome <span class="math notranslate nohighlight">\(y\)</span>, we consider a <strong>distribution over possible outcomes</strong> <span class="math notranslate nohighlight">\(p(y\mid x)\)</span>. Each candidate solution <span class="math notranslate nohighlight">\(y^{(i)}\)</span> carries a probability weight <span class="math notranslate nohighlight">\(p(y^{(i)}\mid x)\)</span> (often written simply as <span class="math notranslate nohighlight">\(p\)</span>), and samples <span class="math notranslate nohighlight">\(\{y^{(i)}\}\)</span> drawn from this distribution should reflect those probabilities so that <strong>rare</strong> and <strong>frequent</strong> outcomes are distinguishable.</p>
<p>In short, rather than reasoning about a single solution <span class="math notranslate nohighlight">\(y\)</span> per input, we reason about <strong>many samples</strong> <span class="math notranslate nohighlight">\(y^{(1)}, y^{(2)}, \dots\)</span> from a distribution that captures both <strong>variability</strong> in the data and <strong>uncertainty</strong> in our model.</p>
<section id="uncertainty">
<h2>Uncertainty<a class="headerlink" href="#uncertainty" title="Link to this heading">#</a></h2>
<p>Every component of our pipelines—measurements, mechanistic models, and numerical solvers—introduces uncertainty. Sensors and clinical assays add <strong>measurement error</strong>. Mathematical models only describe a <strong>subset</strong> of the relevant physiology or physics, leaving the rest unmodeled. Discretization in simulations injects <strong>numerical error</strong>. In learning systems, the trained model itself contributes <strong>approximation error</strong>. Together these sources shape the <strong>predictive uncertainty</strong> of our outputs. For practical decision making, it is essential to <strong>quantify</strong> this uncertainty—one of the core reasons to work with probabilistic models and the central focus of <strong>uncertainty quantification (UQ)</strong>.</p>
<p><strong>Aleatoric vs. Epistemic Uncertainty.</strong> It is common—though not always clean in practice—to separate predictive uncertainty into two broad types:</p>
<ul class="simple">
<li><p><strong>Aleatoric uncertainty</strong> is <strong>inherent variability in the data</strong>, such as sensor noise, stochastic biological processes, or unobserved confounders. It persists even with infinite data.</p></li>
<li><p><strong>Epistemic uncertainty</strong> is <strong>uncertainty about the model</strong>—for example, due to limited training data, misspecified architectures, or poorly explored parameter regions. It can, in principle, be reduced by collecting more informative data.</p></li>
</ul>
<p>A word of caution: this split is <strong>idealized</strong>. Effects may <strong>overlap</strong> and can be hard to disentangle. For instance, what looks like noisy outcomes (aleatoric) could stem from an overly coarse discretization or an inadequate hypothesis class (epistemic). In practice, the two often co-occur.</p>
<p>Closely related is the perspective of <strong>simulation-based inference (SBI)</strong>. Here the emphasis lies on estimating <strong>likelihoods</strong> or <strong>posteriors</strong> for models that are specified by <strong>computer simulations</strong> rather than analytic formulas. SBI offers a principled workflow for marrying simulators with uncertainty and will serve as a guiding thread in what follows.</p>
</section>
<section id="forward-or-backward">
<h2>Forward or Backward?<a class="headerlink" href="#forward-or-backward" title="Link to this heading">#</a></h2>
<p>It is crucial to distinguish <strong>forward</strong> from <strong>inverse</strong> (or “backward”) problems. Traditional numerical methods largely target <strong>forward problems</strong>: given a current state and parameters, compute a steady-state or future state of the system.</p>
<p>Many scientific and clinical questions, however, are <strong>inverse problems</strong>. A forward simulator is still central, but the unknowns are the <strong>inputs or parameters</strong> that best explain observed data. Formally, let a simulator <span class="math notranslate nohighlight">\(\mathcal{S}_\phi\)</span> be parameterized by <span class="math notranslate nohighlight">\(\phi\)</span> (e.g., viscosity, diffusion rates) and act on a state <span class="math notranslate nohighlight">\(x\)</span> to produce an output <span class="math notranslate nohighlight">\(y=\mathcal{S}_\phi(x)\)</span>. We observe <span class="math notranslate nohighlight">\(y_{\text{obs}}\)</span> and wish to infer <span class="math notranslate nohighlight">\(\phi\)</span> (and, at times, parts of <span class="math notranslate nohighlight">\(x\)</span>) such that the simulator’s output matches the observation. In the simplest deterministic setting, this becomes an optimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\phi}\; \mathcal{L}\!\left(\mathcal{S}_\phi(x),\; y_{\text{obs}}\right).
\]</div>
<p>Solving it might reveal, for example, the viscosity of a specimen from rheology data. Similar inverse formulations arise across disciplines—from material science and fluid mechanics to neuroimaging and cosmology.</p>
<p>For clarity of notation, we will often <strong>bundle</strong> any state components we intend to estimate together with the parameters into a single vector <span class="math notranslate nohighlight">\(\theta\)</span>. We then “solve for <span class="math notranslate nohighlight">\(\theta\)</span>” while remembering that <span class="math notranslate nohighlight">\(\theta\)</span> may include physical parameters, initial conditions, boundary terms, or latent states.</p>
<p>In what follows we emphasize <strong>inverse problems</strong> because they showcase the strengths of probabilistic modeling most directly. However, the algorithms we develop are <strong>not limited</strong> to inverse problems; we will point to forward-model applications as well.</p>
</section>
<section id="simulation-based-inference">
<h2>Simulation-Based Inference<a class="headerlink" href="#simulation-based-inference" title="Link to this heading">#</a></h2>
<p>In inverse settings, matching a <strong>single</strong> observation is rarely sufficient. We want a <span class="math notranslate nohighlight">\(\theta\)</span> that explains a <strong>range</strong> of observations, we may entertain <strong>multiple plausible</strong> parameter values, and we often need to quantify how <strong>uncertain</strong> the estimate is. Are the data compatible with a <strong>narrow</strong> range of parameters, or could <span class="math notranslate nohighlight">\(\theta\)</span> vary by <strong>orders of magnitude</strong> without materially degrading the fit? Addressing such questions requires <strong>statistical inference</strong>—characterizing a distribution over <span class="math notranslate nohighlight">\(\theta\)</span> rather than a point estimate.</p>
<p>To connect with the aleatoric/epistemic split: in SBI we are primarily interrogating <strong>uncertainty in the observations given a scientific hypothesis</strong> encoded by a simulator. The simulator’s parameters <span class="math notranslate nohighlight">\(\theta\)</span> are random variables we aim to infer.</p>
<p>Let <span class="math notranslate nohighlight">\(x\)</span> denote the <strong>inputs</strong> to the simulator (which may include known states and controllable settings). Let <span class="math notranslate nohighlight">\(z\)</span> denote <strong>latent variables</strong> capturing unknown or uncontrolled aspects of the system (e.g., unobserved stochasticity, intermediate steps, or random branches in the simulator’s control flow). We adopt the following generative story:</p>
<ul class="simple">
<li><p>Prior over parameters: <span class="math notranslate nohighlight">\(p(\theta)\)</span>.</p></li>
<li><p>Prior over latents: <span class="math notranslate nohighlight">\(p(z)\)</span> (potentially conditioned on <span class="math notranslate nohighlight">\(x\)</span>).</p></li>
<li><p>Simulator outputs: <span class="math notranslate nohighlight">\(y=\mathcal{S}_\theta(x,z)\)</span>.</p></li>
</ul>
<p>We observe <span class="math notranslate nohighlight">\(y_{\text{obs}}\)</span> and seek the <strong>posterior</strong> over parameters</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid x, y_{\text{obs}}) \;\propto\; p(y_{\text{obs}} \mid x, \theta)\, p(\theta).
\]</div>
<p><strong>Bayes’ theorem</strong> underpins everything we do:</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid x, y) \;=\; \frac{p(y \mid x, \theta)\, p(\theta)}{p(y \mid x)}.
\]</div>
<p>Both sides equal the joint <span class="math notranslate nohighlight">\(p(\theta, y \mid x)\)</span> divided by the marginal <span class="math notranslate nohighlight">\(p(y \mid x)\)</span>.</p>
<p>The key building block is the <strong>likelihood</strong></p>
<div class="math notranslate nohighlight">
\[
p(y \mid x, \theta) \;=\; \int p\!\left(y \mid x, \theta, z\right)\, p(z)\, dz,
\]</div>
<p>the probability of seeing <span class="math notranslate nohighlight">\(y\)</span> when the simulator is run at <span class="math notranslate nohighlight">\(\theta\)</span> (marginalizing latent randomness <span class="math notranslate nohighlight">\(z\)</span>). This integral is often <strong>intractable</strong>: <span class="math notranslate nohighlight">\(z\)</span> may be high-dimensional, hard to sample efficiently, or partly determined by opaque control flow. Classic estimators exist—<strong>Approximate Bayesian Computation (ABC)</strong> being a notable family—but they are <strong>computationally heavy</strong>, require careful summary statistics and distances, and suffer from the <strong>curse of dimensionality</strong>.</p>
<p>The denominator <span class="math notranslate nohighlight">\(p(y \mid x)\)</span> is the <strong>evidence</strong> (or marginal likelihood):</p>
<div class="math notranslate nohighlight">
\[
p(y \mid x) \;=\; \int p(y \mid x, \theta)\, p(\theta)\, d\theta.
\]</div>
<p>It normalizes the posterior. While the evidence need not be known to <strong>sample</strong> from the posterior (e.g., via MCMC, which uses ratios that cancel the evidence), estimating it can be useful for <strong>model comparison</strong>. In any case, computing likelihoods and evidences directly is rarely feasible for complex simulators, motivating learned approximations.</p>
</section>
<section id="leveraging-deep-learning">
<h2>Leveraging Deep Learning<a class="headerlink" href="#leveraging-deep-learning" title="Link to this heading">#</a></h2>
<p>This is where deep learning becomes especially powerful. We can <strong>learn a conditional density estimator</strong></p>
<div class="math notranslate nohighlight">
\[
q_\psi(\theta \mid x, y)
\]</div>
<p>that approximates the posterior and supports <strong>fast sampling</strong>. Crucially, we can train it using <strong>simulated pairs</strong> <span class="math notranslate nohighlight">\((\theta, y)\sim p(\theta)\,p(z)\,\delta(y-\mathcal{S}_\theta(x,z))\)</span>, without requiring closed-form likelihoods.</p>
<p>To emphasize the probabilistic role of the network, we switch from writing <span class="math notranslate nohighlight">\(f_\theta\)</span> for deterministic predictors to <span class="math notranslate nohighlight">\(q_\psi\)</span> for <strong>learned densities</strong>. We will often start with <strong>unconditional</strong> density models <span class="math notranslate nohighlight">\(q_\psi(\theta)\)</span> and then extend them to <strong>conditional</strong> forms <span class="math notranslate nohighlight">\(q_\psi(\theta \mid x,y)\)</span>.</p>
<p><strong>Pros of learned SBI approaches:</strong></p>
<ul class="simple">
<li><p><strong>Fast inference</strong> after training: sampling or evaluating <span class="math notranslate nohighlight">\(q_\psi(\theta \mid x, y)\)</span> is cheap.</p></li>
<li><p><strong>Mitigated curse of dimensionality</strong> relative to rejection-based ABC, via amortized, representation-rich models.</p></li>
<li><p><strong>Flexible priors and posteriors:</strong> compatible with complex, multimodal, or heavy-tailed distributions.</p></li>
</ul>
<p><strong>Cons and caveats:</strong></p>
<ul class="simple">
<li><p><strong>Heavy upfront cost:</strong> training requires many simulator runs and careful coverage of the parameter space.</p></li>
<li><p><strong>Approximation gaps:</strong> theoretical guarantees are weaker than in asymptotic MCMC; misspecification or limited simulations can bias the learned posterior.</p></li>
</ul>
<p>We will develop a widely used and remarkably effective family of models—<strong>diffusion-based methods</strong>—for learning such densities. Rather than jumping straight to the final algorithm, we will build it from first principles, as the path introduces several influential ideas in modern machine learning. We will focus on the <strong>core construction</strong> here and return later to physics-aware variants that integrate <strong>differentiable simulators</strong>.</p>
</section>
<section id="learning-a-probability-distribution">
<h2>Learning a Probability Distribution<a class="headerlink" href="#learning-a-probability-distribution" title="Link to this heading">#</a></h2>
<p>A central question in probabilistic modeling is: <strong>how can we learn an unknown probability distribution from data?</strong> When prior knowledge is available, we can posit a <strong>parametric family</strong> (e.g., a Gaussian in the simplest case). Given samples drawn from the target distribution, we then adjust the parameters by minimizing a measure of discrepancy between the target and our parametric approximation. This casts distribution learning as an optimization problem over probability measures.</p>
<section id="fundamentals-a-training-objective">
<h3>Fundamentals: A Training Objective<a class="headerlink" href="#fundamentals-a-training-objective" title="Link to this heading">#</a></h3>
<p>A widely used discrepancy measure is the <strong>Kullback–Leibler (KL) divergence</strong> between two distributions <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> with densities <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> (with respect to a common base measure). It is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{KL}(P\|Q)
\;=\;
\int p(x)\,\log\!\frac{p(x)}{q(x)}\,\mathrm{d}x.
\]</div>
<p>The KL divergence satisfies <span class="math notranslate nohighlight">\(\mathrm{KL}(P\|Q)\ge 0\)</span> and <span class="math notranslate nohighlight">\(\mathrm{KL}(P\|Q)=0\)</span> <strong>if</strong> <span class="math notranslate nohighlight">\(P=Q\)</span> (i.e., <span class="math notranslate nohighlight">\(p=q\)</span> almost everywhere). Thus it acts as a principled “distance-like” objective for fitting distributions.</p>
<p>Suppose we choose a <strong>parametric family</strong> <span class="math notranslate nohighlight">\(\{Q_\theta\}_{\theta\in\Theta}\)</span> with densities <span class="math notranslate nohighlight">\(q_\theta\)</span>. Each <span class="math notranslate nohighlight">\(q_\theta\)</span> must be a <strong>valid density</strong>:</p>
<div class="math notranslate nohighlight">
\[
q_\theta(x)\ge 0
\quad\text{and}\quad
\int q_\theta(x)\,\mathrm{d}x = 1.
\]</div>
<p>Our goal is to pick parameters <span class="math notranslate nohighlight">\(\theta\)</span> so that <span class="math notranslate nohighlight">\(Q_\theta\)</span> is as close as possible to the true data-generating distribution <span class="math notranslate nohighlight">\(P\)</span>. Using KL divergence as the criterion yields</p>
<div class="math notranslate nohighlight">
\[
\theta^\star
\;=\;
\arg\min_{\theta}\;
\mathrm{KL}\!\big(P\;\|\;Q_\theta\big).
\]</div>
<p>Expanding the KL objective gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{KL}\!\big(P\;\|\;Q_\theta\big)
&amp;=
\int p(x)\,\log\!\frac{p(x)}{q_\theta(x)}\,\mathrm{d}x
\\
&amp;=
\underbrace{\int p(x)\,\log p(x)\,\mathrm{d}x}_{\displaystyle \mathbb{E}_{P}[\log p(X)]}
-
\underbrace{\int p(x)\,\log q_\theta(x)\,\mathrm{d}x}_{\displaystyle \mathbb{E}_{P}[\log q_\theta(X)]}
\\[2mm]
&amp;=
\mathbb{E}_{P}[\log p(X)]
-
\mathbb{E}_{P}[\log q_\theta(X)].
\end{aligned}
\end{split}\]</div>
<p>The first term, <span class="math notranslate nohighlight">\(\mathbb{E}_{P}[\log p(X)]\)</span>, does <strong>not</strong> depend on <span class="math notranslate nohighlight">\(\theta\)</span>. Therefore, minimizing <span class="math notranslate nohighlight">\(\mathrm{KL}(P\|Q_\theta)\)</span> over <span class="math notranslate nohighlight">\(\theta\)</span> is <strong>equivalent</strong> to maximizing the expected log-likelihood under <span class="math notranslate nohighlight">\(q_\theta\)</span>, or, equivalently, minimizing the <strong>negative</strong> expected log-likelihood:</p>
<div class="math notranslate nohighlight">
\[
\theta^\star
=
\arg\min_{\theta}\;
\Big(-\,\mathbb{E}_{X\sim P}\big[\log q_\theta(X)\big]\Big).
\]</div>
<p>In practice, the expectation over <span class="math notranslate nohighlight">\(P\)</span> is approximated with data samples <span class="math notranslate nohighlight">\(\{x_i\}_{i=1}^N\sim P\)</span>, leading to the empirical objective</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{L}}(\theta)
=
-\frac{1}{N}\sum_{i=1}^{N}\log q_\theta(x_i),
\]</div>
<p>i.e., <strong>maximum likelihood estimation (MLE)</strong> for the model <span class="math notranslate nohighlight">\(q_\theta\)</span>. Thus, learning a distribution by minimizing the forward KL reduces to fitting parameters that <strong>maximize likelihood</strong> on the observed samples, provided the parametric family always represents valid probability densities.</p>
</section>
</section>
<section id="generative-adversarial-networks">
<h2>Generative Adversarial Networks<a class="headerlink" href="#generative-adversarial-networks" title="Link to this heading">#</a></h2>
<p>We framed generative modeling as the task of representing the <strong>full distribution</strong> over possible states of a variable <span class="math notranslate nohighlight">\(x\)</span>, i.e., learning <span class="math notranslate nohighlight">\(p(x)\)</span> (or <span class="math notranslate nohighlight">\(p(x\mid c)\)</span> when conditioned on inputs <span class="math notranslate nohighlight">\(c\)</span>). Long before diffusion models (DDPMs and relatives) rose to prominence, <strong>Generative Adversarial Networks (GANs)</strong> provided a powerful, if temperamental, route to this goal. Although much current research gravitates toward diffusion-based approaches, GANs remain conceptually elegant and practically useful. This chapter introduces their core ideas, explains how they are trained, and highlights scenarios—especially those with <strong>ambiguous targets</strong> and <strong>no differentiable physics model</strong>—where GANs excel by avoiding the “regression-to-the-mean” pitfall of standard supervised learning.</p>
<section id="maximum-likelihood-estimation">
<h3>Maximum Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h3>
<p>To set the stage, recall <strong>classification</strong> with <span class="math notranslate nohighlight">\(K\)</span> classes. Given a dataset <span class="math notranslate nohighlight">\(\{(x_i, y_i)\}_{i=1}^N\)</span> with <span class="math notranslate nohighlight">\(y_i\in\{1,\dots,K\}\)</span>, a probabilistic classifier <span class="math notranslate nohighlight">\(p_\phi(y\mid x)\)</span> (e.g., a softmax network with parameters <span class="math notranslate nohighlight">\(\phi\)</span>) is typically trained by <strong>maximum likelihood estimation (MLE)</strong>:</p>
<div class="math notranslate nohighlight">
\[
\phi^\star \;=\; \arg\max_{\phi}\; \prod_{i=1}^N p_\phi\!\big(y_i \mid x_i\big)
\;\;\;\Longleftrightarrow\;\;\;
\phi^\star \;=\; \arg\min_{\phi}\; \underbrace{-\sum_{i=1}^N \log p_\phi(y_i\mid x_i)}_{\text{negative log-likelihood (cross-entropy)}}.
\]</div>
<p>This ubiquitous objective admits several equivalent views:</p>
<ul class="simple">
<li><p>It <strong>minimizes</strong> the <strong>KL divergence</strong> between the empirical label distribution <span class="math notranslate nohighlight">\(\hat p(y\mid x)\)</span> and the model <span class="math notranslate nohighlight">\(p_\phi(y\mid x)\)</span>.</p></li>
<li><p>It <strong>maximizes</strong> the empirical expected log-likelihood <span class="math notranslate nohighlight">\(\mathbb{E}_{(x,y)\sim \hat p}\big[\log p_\phi(y\mid x)\big]\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(K=2\)</span> with a sigmoid output, it reduces to the standard <strong>binary cross-entropy</strong>.</p></li>
</ul>
<p>This MLE perspective is central for GANs, where a classifier—the <strong>discriminator</strong>—is trained by (conditional) cross-entropy and then used to <strong>shape</strong> the generator.</p>
</section>
<section id="adversarial-training">
<h3>Adversarial Training<a class="headerlink" href="#adversarial-training" title="Link to this heading">#</a></h3>
<p>A vanilla GAN comprises two networks:</p>
<ul class="simple">
<li><p>A <strong>generator</strong> <span class="math notranslate nohighlight">\(G_\theta\)</span> maps a random latent code <span class="math notranslate nohighlight">\(z\sim p(z)\)</span> (e.g., <span class="math notranslate nohighlight">\(z\sim\mathcal{N}(0,I)\)</span>) to a synthetic sample <span class="math notranslate nohighlight">\(\tilde x=G_\theta(z)\)</span>.</p></li>
<li><p>A <strong>discriminator</strong> <span class="math notranslate nohighlight">\(D_\psi\)</span> maps a sample to a scalar <span class="math notranslate nohighlight">\(D_\psi(x)\in(0,1)\)</span>, interpreted as “probability of being <strong>real</strong>.”</p></li>
</ul>
<p>The canonical <strong>minimax</strong> objective is</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta}\;\max_{\psi}\;\;
\mathbb{E}_{x\sim p_{\text{data}}}\big[\log D_\psi(x)\big]
\;+\;
\mathbb{E}_{z\sim p(z)}\big[\log\big(1 - D_\psi\!\big(G_\theta(z)\big)\big)\big].
\]</div>
<p>Here the discriminator performs <strong>binary MLE</strong>: label real data as 1, generated samples as 0. The generator is trained <strong>through</strong> the discriminator to <strong>fool</strong> it. In practice, one often uses the <strong>non-saturating</strong> generator loss for stronger gradients:</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta}\; \mathbb{E}_{z\sim p(z)}\big[-\log D_\psi\!\big(G_\theta(z)\big)\big].
\]</div>
<p>Training alternates: update <span class="math notranslate nohighlight">\(\psi\)</span> with real and fake batches while holding <span class="math notranslate nohighlight">\(\theta\)</span> fixed, then update <span class="math notranslate nohighlight">\(\theta\)</span> using gradients that flow <strong>through</strong> <span class="math notranslate nohighlight">\(D_\psi\)</span>. Over time, <span class="math notranslate nohighlight">\(G_\theta\)</span> learns to produce samples that are <strong>indistinguishable</strong> from real data according to <span class="math notranslate nohighlight">\(D_\psi\)</span>.</p>
</section>
<section id="regularization">
<h3>Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h3>
<p>Because GANs constitute a <strong>two-player game</strong> rather than a single convex objective, training can be fragile. Imbalances often lead to <strong>mode collapse</strong> (the generator concentrates on a few modes). Stabilization commonly relies on regularization and training heuristics, including:</p>
<ul class="simple">
<li><p><strong>Reconstruction terms</strong> for the generator, e.g., <span class="math notranslate nohighlight">\(\lambda\lVert G_\theta(z) - x\rVert_1\)</span> or <span class="math notranslate nohighlight">\(\lambda\lVert \cdot \rVert_2\)</span> when paired data are available. Pretraining <span class="math notranslate nohighlight">\(G_\theta\)</span> in a supervised fashion can provide a good starting point.</p></li>
<li><p><strong>Feature matching</strong>: penalize <span class="math notranslate nohighlight">\(\ell_2\)</span> distance between intermediate discriminator features on real vs. generated samples, encouraging diverse outputs.</p></li>
<li><p><strong>Gradient penalties / Lipschitz control</strong>: e.g., WGAN-GP or <strong>spectral normalization</strong> in <span class="math notranslate nohighlight">\(D_\psi\)</span> to stabilize the discriminator.</p></li>
<li><p><strong>Label smoothing</strong>, <strong>instance noise</strong>, and <strong>data augmentation</strong> to prevent overconfident discrimination.</p></li>
<li><p><strong>Update scheduling</strong> (e.g., TTUR): different learning rates or step counts for <span class="math notranslate nohighlight">\(D_\psi\)</span> and <span class="math notranslate nohighlight">\(G_\theta\)</span>.</p></li>
</ul>
<p>The overarching goal is to keep the generator–discriminator dynamic <strong>balanced</strong> so that both networks improve without overpowering each other.</p>
</section>
<section id="conditional-gans">
<h3>Conditional GANs<a class="headerlink" href="#conditional-gans" title="Link to this heading">#</a></h3>
<p>For many scientific and engineering problems we do not want to sample unconditionally from <span class="math notranslate nohighlight">\(p(x)\)</span>; we need samples <strong>conditioned</strong> on inputs <span class="math notranslate nohighlight">\(c\)</span> (e.g., parameters, boundary conditions, low-resolution measurements). <strong>Conditional GANs (cGANs)</strong> incorporate <span class="math notranslate nohighlight">\(c\)</span> into both networks:</p>
<div class="math notranslate nohighlight">
\[
\min_{\theta}\;\max_{\psi}\;\;
\mathbb{E}_{(x,c)\sim p_{\text{data}}}\big[\log D_\psi(x, c)\big]
\;+\;
\mathbb{E}_{z\sim p(z),\, c\sim p(c)}\big[\log\big(1 - D_\psi(G_\theta(z,c), c)\big)\big].
\]</div>
<p>A common addition is a <strong>task loss</strong> that ties the output to the condition, e.g., for <strong>super-resolution</strong></p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}_{\text{task}} = \lambda\, \lVert G_\theta(z,\, x_{\text{LR}}) - x_{\text{HR}}\rVert_1,
\]</div>
<p>combined with the adversarial term. The discriminator now judges <strong>consistency</strong> between the condition <span class="math notranslate nohighlight">\(c\)</span> and the output, while the task loss preserves <strong>fidelity</strong> to known measurements.</p>
</section>
<section id="ambiguous-solutions">
<h3>Ambiguous Solutions<a class="headerlink" href="#ambiguous-solutions" title="Link to this heading">#</a></h3>
<p>Supervised regression with <span class="math notranslate nohighlight">\(\ell_1/\ell_2\)</span> losses is prone to <strong>averaging</strong> when the mapping is <strong>multi-modal</strong>—precisely the case for super-resolution, deconvolution, or ill-posed inverse problems. A single low-resolution input <span class="math notranslate nohighlight">\(x_{\text{LR}}\)</span> may correspond to many high-resolution solutions <span class="math notranslate nohighlight">\(x_{\text{HR}}\)</span>. Minimizing MSE encourages the <strong>conditional mean</strong>, which can be <strong>blurry</strong> and <strong>unphysical</strong>.</p>
<p>GANs combat this by <strong>learning the conditional data distribution</strong> <span class="math notranslate nohighlight">\(p(x_{\text{HR}}\mid x_{\text{LR}})\)</span>. The discriminator rewards <strong>realistic</strong> high-frequency structure, nudging the generator to commit to <strong>plausible modes</strong> instead of averaging across them.</p>
</section>
<section id="spatio-temporal-super-resolution">
<h3>Spatio-Temporal Super-Resolution<a class="headerlink" href="#spatio-temporal-super-resolution" title="Link to this heading">#</a></h3>
<p>Ambiguity is not only spatial. In dynamical systems, plausible futures can <strong>branch</strong>, and enforcing <strong>temporal coherence</strong> is crucial. Extending GANs to sequences is natural:</p>
<ul class="simple">
<li><p>Use a <strong>temporal discriminator</strong> <span class="math notranslate nohighlight">\(D_\psi\)</span> that ingests <strong>clips</strong> or <strong>frame triplets</strong> <span class="math notranslate nohighlight">\((x_{t-1}, x_t, x_{t+1})\)</span> (or 3D convolutions over space–time) to judge whether the <strong>evolution</strong> looks realistic.</p></li>
<li><p>Condition the generator on past frames and low-resolution sequences, <span class="math notranslate nohighlight">\(G_\theta(z, \{x_{\text{LR},\tau}\}_{\tau\le t})\)</span>, and jointly optimize adversarial and reconstruction terms.</p></li>
</ul>
<p>Comparisons of <strong>time derivatives</strong> or other physics-motivated quantities often show that spatio-temporal GANs better capture dynamics than frame-wise models, aligning more closely with high-fidelity references.</p>
</section>
</section>
<section id="learning-distributions-with-normalizing-flows">
<h2>Learning Distributions with Normalizing Flows<a class="headerlink" href="#learning-distributions-with-normalizing-flows" title="Link to this heading">#</a></h2>
<p>GANs are prominent generative models and provide us a sampling mechanism for generating new data. However, they do not explicitly learn the probability density function <span class="math notranslate nohighlight">\(p(x)\)</span> of the real input data.
<strong>Normalizing Flows</strong>, on the other hand, actually model the true data distribution and provides us with an exact likelihood estimate.
The key idea is to use a sequence of invertible and differentiable mappings as layers for the neural network.</p>
<p><img alt="image.png" src="chapters/attachment:image.png" /></p>
<section id="invertible-mappings">
<h3>Invertible mappings<a class="headerlink" href="#invertible-mappings" title="Link to this heading">#</a></h3>
<p>Flows construct an <strong>invertible</strong> transformation</p>
<div class="math notranslate nohighlight">
\[
f:\ \mathcal{X}\to\mathcal{Z},\qquad z=f(x)
\]</div>
<p>from data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to a latent space <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> endowed with a simple prior <span class="math notranslate nohighlight">\(p_Z(z)\)</span> (e.g., standard Gaussian). Invertibility enforces <strong>equal dimensionality</strong>: <span class="math notranslate nohighlight">\(\dim(x)=\dim(z)\)</span>. Unlike VAEs—where <span class="math notranslate nohighlight">\(\dim(z)\ll \dim(x)\)</span> is common—flows maintain a <strong>bijective</strong> mapping, enabling <strong>lossless reconstruction</strong>: for each sample <span class="math notranslate nohighlight">\(x\)</span> there is a unique <span class="math notranslate nohighlight">\(z=f(x)\)</span> and vice versa <span class="math notranslate nohighlight">\(x=f^{-1}(z)\)</span>. In the schematic above, this implies <strong>zero reconstruction error</strong> for flows, regardless of the specific invertible <span class="math notranslate nohighlight">\(f\)</span> and input <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="exact-density-via-change-of-variables">
<h3>Exact density via change of variables<a class="headerlink" href="#exact-density-via-change-of-variables" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(z=f(x)\)</span> with a differentiable bijection <span class="math notranslate nohighlight">\(f\)</span>. The <strong>change-of-variables</strong> formula gives the induced data density:</p>
<ul>
<li><p><strong>Univariate</strong>:</p>
<div class="math notranslate nohighlight">
\[
  p_X(x) \;=\; p_Z\!\big(f(x)\big)\;\Bigl|\tfrac{\mathrm{d}}{\mathrm{d}x}f(x)\Bigr|.
  \]</div>
</li>
<li><p><strong>Multivariate</strong> (<span class="math notranslate nohighlight">\(x\in\mathbb{R}^d\)</span>):</p>
<div class="math notranslate nohighlight">
\[
  p_X(x) \;=\; p_Z\!\big(f(x)\big)\;\Bigl|\det J_f(x)\Bigr|,
  \quad
  J_f(x) \equiv \frac{\partial f(x)}{\partial x}\in\mathbb{R}^{d\times d}.
  \]</div>
</li>
</ul>
<p>Taking logs (the usual training objective),</p>
<div class="math notranslate nohighlight">
\[
\log p_X(x) \;=\; \log p_Z\!\big(f(x)\big) \;+\; \log\Bigl|\det J_f(x)\Bigr|.
\]</div>
<p>Thus, evaluating likelihoods reduces to (i) mapping <span class="math notranslate nohighlight">\(x\mapsto z\)</span> and scoring under <span class="math notranslate nohighlight">\(p_Z\)</span>, and (ii) computing <span class="math notranslate nohighlight">\(\log|\det J_f(x)|\)</span>.</p>
</section>
<section id="transforming-densities-and-volume-correction">
<h3>Transforming densities and volume correction<a class="headerlink" href="#transforming-densities-and-volume-correction" title="Link to this heading">#</a></h3>
<p>View the flow <strong>backwards</strong>: start from a simple density <span class="math notranslate nohighlight">\(p_Z\)</span> and push it through an invertible map <span class="math notranslate nohighlight">\(g=f^{-1}\)</span> to synthesize data <span class="math notranslate nohighlight">\(x=g(z)\)</span>. Any invertible transformation <strong>warps</strong> the density while preserving total probability. For a simple translation <span class="math notranslate nohighlight">\(g(z)=z+1\)</span>, the shape shifts without volume change. For a scaling <span class="math notranslate nohighlight">\(g(z)=2z\)</span>, volumes change and heights adjust correspondingly; the <strong>Jacobian determinant</strong> provides the exact volume correction (figure credit: Eric Jang).</p>
<p>As transformations become more expressive, computing <span class="math notranslate nohighlight">\(g^{-1}\)</span> and <span class="math notranslate nohighlight">\(\log|\det J_g|\)</span> can become costly. Flows resolve this by <strong>stacking</strong> many simple, tractable, invertible maps:</p>
<div class="math notranslate nohighlight">
\[
h_0=x,\quad h_k=f_k(h_{k-1}),\quad z=h_K,
\]</div>
<p>with each <span class="math notranslate nohighlight">\(f_k\)</span> designed so that <span class="math notranslate nohighlight">\(f_k^{-1}\)</span> and <span class="math notranslate nohighlight">\(\log|\det J_{f_k}|\)</span> are cheap. The total log-likelihood decomposes as</p>
<div class="math notranslate nohighlight">
\[
\log p_X(x)
\;=\;
\log p_Z(h_K) \;+\; \sum_{k=1}^{K}\log\bigl|\det J_{f_k}(h_{k-1})\bigr|.
\]</div>
<p>Planar and radial flows are early neural parameterizations of <span class="math notranslate nohighlight">\(f_k\)</span>. For high-dimensional data such as images, <strong>coupling layers</strong> (e.g., RealNVP) and <strong>autoregressive</strong> layers (e.g., MAF), often with <strong>invertible <span class="math notranslate nohighlight">\(1\times 1\)</span> convolutions</strong> (Glow), provide scalable, tractable Jacobians.</p>
</section>
<section id="flows-for-images">
<h3>Flows for images<a class="headerlink" href="#flows-for-images" title="Link to this heading">#</a></h3>
<p>For image modeling, a flow maps an input image (e.g., MNIST) to a <strong>same-shaped</strong> latent tensor. This preserves exact invertibility and enables:</p>
<ul class="simple">
<li><p><strong>Density estimation</strong>: forward pass <span class="math notranslate nohighlight">\(x\to z\)</span>, compute <span class="math notranslate nohighlight">\(\log p_Z(z)+\sum\log|\det J|\)</span>.</p></li>
<li><p><strong>Sampling</strong>: draw <span class="math notranslate nohighlight">\(z\sim p_Z\)</span>, then invert <span class="math notranslate nohighlight">\(z\to x=f^{-1}(z)\)</span>.</p></li>
</ul>
<p>A practical flow implementation organizes:</p>
<ul class="simple">
<li><p>a forward routine that returns <span class="math notranslate nohighlight">\(\log p_X(x)\)</span> (or NLL),</p></li>
<li><p>an inverse routine for sampling,</p></li>
<li><p>stable preprocessing (e.g., dequantization for discrete pixels),</p></li>
<li><p>layers with tractable <span class="math notranslate nohighlight">\(\log|\det J|\)</span> and efficient inverses.</p></li>
</ul>
<p><img alt="flowimages" src="../_images/image-2.png" /></p>
</section>
<section id="training-validation-and-inference">
<h3>Training, validation, and inference<a class="headerlink" href="#training-validation-and-inference" title="Link to this heading">#</a></h3>
<p>During training and validation, flows operate in the <strong>forward</strong> direction to maximize the exact log-likelihood (equivalently, minimize NLL). At inference:</p>
<ul class="simple">
<li><p><strong>Likelihood queries</strong> evaluate sample plausibility.</p></li>
<li><p><strong>Generation</strong> uses the inverse path to synthesize new samples.</p></li>
<li><p><strong>Latent manipulations</strong> are possible because <span class="math notranslate nohighlight">\(f\)</span> is bijective.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="27_transformers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Attention Mechanisms and Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="29_PBDL.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Physics-Based Deep Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#uncertainty">Uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-or-backward">Forward or Backward?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#simulation-based-inference">Simulation-Based Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#leveraging-deep-learning">Leveraging Deep Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-a-probability-distribution">Learning a Probability Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamentals-a-training-objective">Fundamentals: A Training Objective</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-adversarial-networks">Generative Adversarial Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adversarial-training">Adversarial Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">Regularization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-gans">Conditional GANs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ambiguous-solutions">Ambiguous Solutions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spatio-temporal-super-resolution">Spatio-Temporal Super-Resolution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-distributions-with-normalizing-flows">Learning Distributions with Normalizing Flows</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invertible-mappings">Invertible mappings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exact-density-via-change-of-variables">Exact density via change of variables</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transforming-densities-and-volume-correction">Transforming densities and volume correction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flows-for-images">Flows for images</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-validation-and-inference">Training, validation, and inference</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Biagio Mandracchia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>