
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Training a Digit Classifier &#8212; Introduction to Machine Learning for Biomedical Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/24_Loss_Metrics_and_Optimizers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Neural Networks as Universal Approximators" href="23_NN_as_UA.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Machine Learning for Biomedical Data - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Machine Learning for Biomedical Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Computational Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_python_primer.html">Python: a Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_data_science_with_pandas.html">Data Science with Python and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_jupyter_markdown.html">Jupyter &amp; Markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Feature Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_machine_learning_fundamentals.html">The Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_basic_feature_engineering.html">Basic Feature Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_categorical_variables.html">Categorical Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_classifiers.html">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_dimensionality_reduction.html">Dimensionality Reduction: PCA</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_kmeans.html">Nonlinear Manifold Feature Extraction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="21_neural%20networks.html">Deep Learning in Biomedical Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_NN_as_UA.html">Neural Networks as Universal Approximators</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Training a Digit Classifier</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD/issues/new?title=Issue%20on%20page%20%2Fchapters/24_Loss_Metrics_and_Optimizers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/24_Loss_Metrics_and_Optimizers.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Training a Digit Classifier</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixels-the-foundations-of-computer-vision">Pixels: The Foundations of Computer Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixel-similarity">Pixel Similarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-metrics-using-broadcasting">Computing Metrics Using Broadcasting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tensors-and-broadcasting">PyTorch Tensors and Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-distance-function-with-broadcasting">Defining a Distance Function with Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-an-image-with-distance">Classifying an Image with Distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-accuracy">Calculating Accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-loss-function">Defining a Loss Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-use-accuracy-as-a-loss-function">Why Not Use Accuracy as a Loss Function?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-loss-function-for-binary-classification">Defining a Loss Function for Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-simple-loss-function">Implementing a Simple Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ensuring-predictions-are-between-0-and-1-with-the-sigmoid-function">Ensuring Predictions are Between 0 and 1 with the Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-loss-function-with-sigmoid">Updating the Loss Function with Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-a-separate-loss-function">Why Do We Need a Separate Loss Function?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-stochastic-gradient-descent-sgd-and-mini-batches">Optimization with Stochastic Gradient Descent (SGD) and Mini-Batches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-mini-batches">Why Use Mini-Batches?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-batch-size">Choosing a Batch Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-the-data">Shuffling the Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-mini-batches-to-images-and-labels">Applying Mini-Batches to Images and Labels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-sgd-step-with-mini-batches">Implementing an SGD Step with Mini-Batches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-parameters-with-optimizer-step">Updating Parameters with <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-accuracy-calculation">Mini-Batch Accuracy Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">Creating an Optimizer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-an-optimizer">Why Do We Need an Optimizer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-optimizers-in-deep-learning">Common Optimizers in Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-basic-optimizer">Custom Basic Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-basicoptim-with-a-model">Using <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> with a Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorchs-built-in-optimizers">Using PyTorch’s Built-in Optimizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#switching-to-the-adam-optimizer">Switching to the Adam Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-fastais-learner-with-optimizers">Using fastai’s <code class="docutils literal notranslate"><span class="pre">Learner</span></code> with Optimizers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-a-nonlinearity">Adding a Nonlinearity</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#hide</span>
<span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">matplotlib</span><span class="o">.</span><span class="n">rc</span><span class="p">(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Greys&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="training-a-digit-classifier">
<h1>Training a Digit Classifier<a class="headerlink" href="#training-a-digit-classifier" title="Link to this heading">#</a></h1>
<p>We’ll explore the steps involved in building a simple digit classifier. We will:</p>
<ul class="simple">
<li><p>Discuss the role of arrays and tensors, and introduce broadcasting, a powerful technique that makes operations on these data structures more expressive.</p></li>
<li><p>Define a suitable loss function for this classification task, and introduce the concept of mini-batches to optimize the training process.</p></li>
<li><p>Explain the mathematical operations performed by a basic neural network.</p></li>
</ul>
<p>By the end, we’ll combine all these pieces to create a model capable of classifying images of digits.</p>
<section id="pixels-the-foundations-of-computer-vision">
<h2>Pixels: The Foundations of Computer Vision<a class="headerlink" href="#pixels-the-foundations-of-computer-vision" title="Link to this heading">#</a></h2>
<p>Our goal is to build a model that can classify an image as either a “3” or a “7.” To start, let’s download a sample of the MNIST dataset that includes images of just these digits:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the MNIST sample dataset</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST_SAMPLE</span><span class="p">)</span>

<span class="c1"># Check the directory contents</span>
<span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(#3) [Path(&#39;C:/Users/biagi/.fastai/data/mnist_sample/labels.csv&#39;),Path(&#39;C:/Users/biagi/.fastai/data/mnist_sample/train&#39;),Path(&#39;C:/Users/biagi/.fastai/data/mnist_sample/valid&#39;)]
</pre></div>
</div>
</div>
</div>
<p>The MNIST dataset follows a common layout for machine learning datasets, with separate folders for the training and validation sets. Inside the training set, there are folders named for each label (3 and 7), which contain the images for each respective digit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(#2) [Path(&#39;C:/Users/biagi/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;C:/Users/biagi/.fastai/data/mnist_sample/train/7&#39;)]
</pre></div>
</div>
</div>
</div>
<p>In this dataset, “3” and “7” are the <em>labels</em> or <em>targets</em>. Let’s take a look at a sample image of a handwritten “3”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the path for the images of &quot;3&quot;</span>
<span class="n">threes</span> <span class="o">=</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="o">/</span><span class="s1">&#39;3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span><span class="o">.</span><span class="n">sorted</span><span class="p">()</span>

<span class="c1"># Open and display an example image</span>
<span class="n">im3_path</span> <span class="o">=</span> <span class="n">threes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">im3</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">im3_path</span><span class="p">)</span>
<span class="n">im3</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Images in Python are often represented by the <code class="docutils literal notranslate"><span class="pre">Image</span></code> class from the <em>Python Imaging Library</em> (PIL). Converting an image to a numerical format, like a NumPy array or PyTorch tensor, allows us to perform mathematical operations directly on the image data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">(</span><span class="n">im3</span><span class="p">)[</span><span class="mi">4</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[  0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,  29],
       [  0,   0,   0,  48, 166, 224],
       [  0,  93, 244, 249, 253, 187],
       [  0, 107, 253, 253, 230,  48],
       [  0,   3,  20,  20,  15,   0]], dtype=uint8)
</pre></div>
</div>
</div>
</div>
<p>Each pixel in the image has a value between 0 and 255, where 0 represents black and 255 represents white, with shades of gray in between. Converting the image to a PyTorch tensor allows us to manipulate and process it further:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="n">im3</span><span class="p">)[</span><span class="mi">4</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[  0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  29],
        [  0,   0,   0,  48, 166, 224],
        [  0,  93, 244, 249, 253, 187],
        [  0, 107, 253, 253, 230,  48],
        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)
</pre></div>
</div>
</div>
</div>
</section>
<section id="pixel-similarity">
<h2>Pixel Similarity<a class="headerlink" href="#pixel-similarity" title="Link to this heading">#</a></h2>
<p>As a simple baseline model, let’s calculate the average pixel values for all images of “3” and “7” separately. This will give us an “ideal” version of each digit, which we can use for comparison. To classify an image, we’ll check which of these ideal representations the image most closely resembles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load images of 3s and 7s as tensors</span>
<span class="n">three_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">threes</span><span class="p">]</span>
<span class="n">seven_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train&#39;</span><span class="o">/</span><span class="s1">&#39;7&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span><span class="o">.</span><span class="n">sorted</span><span class="p">()]</span>

<span class="c1"># Stack tensors and normalize</span>
<span class="n">stacked_threes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">three_tensors</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">stacked_sevens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">seven_tensors</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>

<span class="c1"># Calculate the average image (mean) for each digit</span>
<span class="n">mean3</span> <span class="o">=</span> <span class="n">stacked_threes</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean7</span> <span class="o">=</span> <span class="n">stacked_sevens</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Display the &quot;ideal&quot; 3 and 7</span>
<span class="n">show_image</span><span class="p">(</span><span class="n">mean3</span><span class="p">)</span>
<span class="n">show_image</span><span class="p">(</span><span class="n">mean7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: &gt;
</pre></div>
</div>
<img alt="../_images/4569b2202bd8b80d74aca0ab08ff89dd1018ac6acec4123b028d451d7f02e797.png" src="../_images/4569b2202bd8b80d74aca0ab08ff89dd1018ac6acec4123b028d451d7f02e797.png" />
<img alt="../_images/7b8855895150e0b00ff82792e718e52cdfee98d4b2ca8c400a6516909e3d643c.png" src="../_images/7b8855895150e0b00ff82792e718e52cdfee98d4b2ca8c400a6516909e3d643c.png" />
</div>
</div>
<p>This approach provides a very basic model: for any new image, we calculate its “distance” from the average 3 and average 7, then classify it as the closest match. Distance can be calculated using either:</p>
<ul class="simple">
<li><p><strong>Mean Absolute Difference</strong> (L1 norm)</p></li>
<li><p><strong>Root Mean Squared Error</strong> (L2 norm)</p></li>
</ul>
<p>If the distance to the ideal 3 is smaller, we classify the image as a 3, otherwise as a 7.</p>
<p>Let’s try both of these now:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a_3</span> <span class="o">=</span> <span class="n">stacked_threes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">show_image</span><span class="p">(</span><span class="n">a_3</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/88e210254e9ff0365cd9e49e2357efcf7be2dc2a02547f1bb13cc395a1a318be.png" src="../_images/88e210254e9ff0365cd9e49e2357efcf7be2dc2a02547f1bb13cc395a1a318be.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist_3_abs</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_3</span> <span class="o">-</span> <span class="n">mean3</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">dist_3_sqr</span> <span class="o">=</span> <span class="p">((</span><span class="n">a_3</span> <span class="o">-</span> <span class="n">mean3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<span class="n">dist_3_abs</span><span class="p">,</span><span class="n">dist_3_sqr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.1114), tensor(0.2021))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dist_7_abs</span> <span class="o">=</span> <span class="p">(</span><span class="n">a_3</span> <span class="o">-</span> <span class="n">mean7</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">dist_7_sqr</span> <span class="o">=</span> <span class="p">((</span><span class="n">a_3</span> <span class="o">-</span> <span class="n">mean7</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
<span class="n">dist_7_abs</span><span class="p">,</span><span class="n">dist_7_sqr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.1586), tensor(0.3021))
</pre></div>
</div>
</div>
</div>
<p>In both cases, the distance between our 3 and the “ideal” 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case.</p>
<blockquote>
<div><p>Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes).</p>
</div></blockquote>
</section>
<section id="computing-metrics-using-broadcasting">
<h2>Computing Metrics Using Broadcasting<a class="headerlink" href="#computing-metrics-using-broadcasting" title="Link to this heading">#</a></h2>
<p>To evaluate the performance of our model, we need a way to measure how well it’s doing. This measurement is called a <strong>metric</strong>. Metrics are calculated based on the model’s predictions compared to the actual labels in the dataset. For classification tasks like this, a common metric is <strong>accuracy</strong>.</p>
<p>In our case, we want to determine if each image is classified correctly as either a “3” or a “7.” Accuracy for this task can be computed by:</p>
<ol class="arabic simple">
<li><p>Calculating the model’s prediction for each image.</p></li>
<li><p>Checking if the prediction matches the actual label.</p></li>
<li><p>Averaging these correct/incorrect results across the entire dataset.</p></li>
</ol>
<p>We have two sets of validation images: one for 3s and one for 7s. Let’s load and prepare these validation sets as tensors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load and normalize validation sets</span>
<span class="n">valid_3_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;valid&#39;</span><span class="o">/</span><span class="s1">&#39;3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>
<span class="n">valid_7_tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">tensor</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">o</span><span class="p">))</span> <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;valid&#39;</span><span class="o">/</span><span class="s1">&#39;7&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">ls</span><span class="p">()])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">/</span><span class="mi">255</span>

<span class="c1"># Check shapes of validation tensors</span>
<span class="n">valid_3_tens</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">valid_7_tens</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))
</pre></div>
</div>
</div>
</div>
<p>Each tensor has the shape <code class="docutils literal notranslate"><span class="pre">[number_of_images,</span> <span class="pre">height,</span> <span class="pre">width]</span></code>, where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">valid_3_tens</span></code> has 1,010 images of 3s.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">valid_7_tens</span></code> has 1,028 images of 7s.</p></li>
</ul>
<p>This setup allows us to test our function across both categories and compute accuracy based on all validation images.</p>
<section id="pytorch-tensors-and-broadcasting">
<h3>PyTorch Tensors and Broadcasting<a class="headerlink" href="#pytorch-tensors-and-broadcasting" title="Link to this heading">#</a></h3>
<p>Tensors are multidimensional arrays that enable fast computation in PyTorch. Unlike Python lists, which are slow, tensors allow us to perform large-scale mathematical operations quickly.</p>
<p>For instance, let’s add 1 to each element in a tensor and multiply it by another tensor, without any Python loops:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tns</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">tns</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">tns</span> <span class="o">*</span> <span class="mf">1.5</span>
</pre></div>
</div>
<p>Broadcasting is a powerful technique that allows us to perform operations between tensors of different shapes. For example, subtracting an “ideal” digit from each image in a batch is possible because PyTorch will “broadcast” the ideal digit to match the batch size.</p>
</section>
<section id="defining-a-distance-function-with-broadcasting">
<h3>Defining a Distance Function with Broadcasting<a class="headerlink" href="#defining-a-distance-function-with-broadcasting" title="Link to this heading">#</a></h3>
<p>To classify an image, we need to calculate its “distance” from the “ideal” 3 and 7 (the averages we calculated earlier). A useful function for calculating this distance is <strong>mean absolute error</strong> (MAE).</p>
<p>With broadcasting, we can apply the same distance calculation to an entire set of images without writing loops. Here’s our distance function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define distance function using broadcasting</span>
<span class="k">def</span> <span class="nf">mnist_distance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Calculate distance from an example image to the ideal 3</span>
<span class="n">a_3</span> <span class="o">=</span> <span class="n">valid_3_tens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">mnist_distance</span><span class="p">(</span><span class="n">a_3</span><span class="p">,</span> <span class="n">mean3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.1280)
</pre></div>
</div>
</div>
</div>
<p>When we pass <code class="docutils literal notranslate"><span class="pre">valid_3_tens</span></code> (all 3s in the validation set) and <code class="docutils literal notranslate"><span class="pre">mean3</span></code> (the ideal 3) to this function, broadcasting automatically expands <code class="docutils literal notranslate"><span class="pre">mean3</span></code> to match the shape of <code class="docutils literal notranslate"><span class="pre">valid_3_tens</span></code>, calculating the distance for each image in the set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate distance for all 3s in the validation set</span>
<span class="n">valid_3_dist</span> <span class="o">=</span> <span class="n">mnist_distance</span><span class="p">(</span><span class="n">valid_3_tens</span><span class="p">,</span> <span class="n">mean3</span><span class="p">)</span>
<span class="n">valid_3_dist</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># Output should be a vector of distances</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([1010])
</pre></div>
</div>
</div>
</div>
<p>Broadcasting makes this possible by “stretching” <code class="docutils literal notranslate"><span class="pre">mean3</span></code> to align with each image in <code class="docutils literal notranslate"><span class="pre">valid_3_tens</span></code>, allowing the distance function to operate on all images simultaneously. This avoids the need for loops and makes the computation faster and more efficient.</p>
</section>
<section id="classifying-an-image-with-distance">
<h3>Classifying an Image with Distance<a class="headerlink" href="#classifying-an-image-with-distance" title="Link to this heading">#</a></h3>
<p>To classify a new image, we compare its distance to <code class="docutils literal notranslate"><span class="pre">mean3</span></code> and <code class="docutils literal notranslate"><span class="pre">mean7</span></code>. If it’s closer to <code class="docutils literal notranslate"><span class="pre">mean3</span></code>, we classify it as a 3; if it’s closer to <code class="docutils literal notranslate"><span class="pre">mean7</span></code>, we classify it as a 7:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define classification function</span>
<span class="k">def</span> <span class="nf">is_3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">mnist_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean3</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mnist_distance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean7</span><span class="p">)</span>

<span class="c1"># Test classification on a sample 3</span>
<span class="n">is_3</span><span class="p">(</span><span class="n">a_3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(True)
</pre></div>
</div>
</div>
</div>
<p>For the validation set, we can use broadcasting to classify every image at once:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Classify all 3s in the validation set</span>
<span class="n">is_3</span><span class="p">(</span><span class="n">valid_3_tens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ True,  True,  True,  ..., False,  True,  True])
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-accuracy">
<h3>Calculating Accuracy<a class="headerlink" href="#calculating-accuracy" title="Link to this heading">#</a></h3>
<p>To compute accuracy, we calculate the proportion of correct predictions:</p>
<ul class="simple">
<li><p>For images of 3s, we check if the model predicted 3 (output is <code class="docutils literal notranslate"><span class="pre">True</span></code>).</p></li>
<li><p>For images of 7s, we check if the model predicted 7 (output is <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p></li>
</ul>
<p>Here’s how to calculate accuracy for each set and the overall accuracy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate accuracy for each digit</span>
<span class="n">accuracy_3s</span> <span class="o">=</span> <span class="n">is_3</span><span class="p">(</span><span class="n">valid_3_tens</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Accuracy on 3s</span>
<span class="n">accuracy_7s</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">is_3</span><span class="p">(</span><span class="n">valid_7_tens</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Accuracy on 7s</span>

<span class="c1"># Calculate overall accuracy</span>
<span class="n">overall_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">accuracy_3s</span> <span class="o">+</span> <span class="n">accuracy_7s</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">accuracy_3s</span><span class="p">,</span> <span class="n">accuracy_7s</span><span class="p">,</span> <span class="n">overall_accuracy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.9168), tensor(0.9854), tensor(0.9511))
</pre></div>
</div>
</div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">accuracy_3s</span></code> and <code class="docutils literal notranslate"><span class="pre">accuracy_7s</span></code> represent the accuracy for classifying 3s and 7s, respectively. By averaging these, we get an overall measure of the model’s performance.</p>
</section>
</section>
<section id="defining-a-loss-function">
<h2>Defining a Loss Function<a class="headerlink" href="#defining-a-loss-function" title="Link to this heading">#</a></h2>
<p>In deep learning, the loss function is a critical component used to guide the training process. It quantifies how well our model’s predictions match the target values (labels), allowing us to improve the model by minimizing this loss over successive iterations.</p>
<p>Our goal here is to create a loss function that:</p>
<ol class="arabic simple">
<li><p>Produces a value reflecting how far the model’s predictions are from the true labels.</p></li>
<li><p>Has a gradient that can guide the optimization process, making it possible to update the model’s parameters effectively.</p></li>
</ol>
<section id="why-not-use-accuracy-as-a-loss-function">
<h3>Why Not Use Accuracy as a Loss Function?<a class="headerlink" href="#why-not-use-accuracy-as-a-loss-function" title="Link to this heading">#</a></h3>
<p>A straightforward choice for measuring model performance is <strong>accuracy</strong>, which calculates the proportion of correct predictions. However, accuracy is unsuitable as a loss function for two reasons:</p>
<ol class="arabic simple">
<li><p><strong>Flat gradients</strong>: Small changes in weights often do not affect accuracy, because the predictions remain classified the same way (correct or incorrect). This causes “flat” gradients, making it difficult to improve the model.</p></li>
<li><p><strong>Discrete metric</strong>: Accuracy is a discrete measure that doesn’t capture “how wrong” a prediction is. For instance, if the model is very close to the correct answer, accuracy won’t reflect this, but a good loss function should.</p></li>
</ol>
<p>For these reasons, accuracy is often used as a <strong>metric</strong> (to evaluate the model’s performance), while the <strong>loss function</strong> drives the learning process.</p>
</section>
<section id="defining-a-loss-function-for-binary-classification">
<h3>Defining a Loss Function for Binary Classification<a class="headerlink" href="#defining-a-loss-function-for-binary-classification" title="Link to this heading">#</a></h3>
<p>In our MNIST task, we are trying to classify images as either a “3” or a “7.” We can treat this as a binary classification problem:</p>
<ul class="simple">
<li><p><strong>1</strong> for images of “3”.</p></li>
<li><p><strong>0</strong> for images of “7”.</p></li>
</ul>
<p>Our goal is to ensure that the model outputs values close to 1 for images of “3” and values close to 0 for images of “7.” A common approach in binary classification is to use the <strong>mean absolute error</strong> or <strong>cross-entropy loss</strong>.</p>
<p>In this example, we’ll use a custom function that measures the distance between the model’s predictions and the true labels, ensuring that:</p>
<ul class="simple">
<li><p>For images of “3” (target 1), the prediction is close to 1.</p></li>
<li><p>For images of “7” (target 0), the prediction is close to 0.</p></li>
</ul>
</section>
<section id="implementing-a-simple-loss-function">
<h3>Implementing a Simple Loss Function<a class="headerlink" href="#implementing-a-simple-loss-function" title="Link to this heading">#</a></h3>
<p>Suppose we have three images with labels <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0,</span> <span class="pre">1]</span></code> (representing “3,” “7,” and “3”). Let’s say our model’s predictions are <code class="docutils literal notranslate"><span class="pre">[0.9,</span> <span class="pre">0.4,</span> <span class="pre">0.2]</span></code>. These predictions mean:</p>
<ul class="simple">
<li><p>The model is confident the first image is a 3.</p></li>
<li><p>The model is slightly confident that the second image is a 7.</p></li>
<li><p>The model incorrectly predicts the third image as a 7.</p></li>
</ul>
<p>Here’s a simple loss function to measure how far the predictions are from the target labels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define sample targets and predictions</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="c1"># Define the MNIST loss function</span>
<span class="k">def</span> <span class="nf">mnist_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>In this function:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.where(targets</span> <span class="pre">==</span> <span class="pre">1,</span> <span class="pre">1</span> <span class="pre">-</span> <span class="pre">predictions,</span> <span class="pre">predictions)</span></code> calculates the distance between each prediction and the target (1 or 0).</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">targets</span> <span class="pre">==</span> <span class="pre">1</span></code> (for a 3), we calculate <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">prediction</span></code> to measure how close the prediction is to 1.</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">targets</span> <span class="pre">==</span> <span class="pre">0</span></code> (for a 7), we simply use <code class="docutils literal notranslate"><span class="pre">prediction</span></code> to measure how close it is to 0.</p></li>
<li><p>We then take the mean of all these distances to get the overall loss.</p></li>
</ul>
<p>Let’s test it with our example values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate loss</span>
<span class="n">mnist_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.4333)
</pre></div>
</div>
</div>
</div>
<p>A lower loss indicates that the predictions are closer to the targets. For instance, if we change the prediction for the third image from <code class="docutils literal notranslate"><span class="pre">0.2</span></code> to <code class="docutils literal notranslate"><span class="pre">0.8</span></code>, which is closer to the target of 1, the loss decreases:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mnist_loss</span><span class="p">(</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]),</span> <span class="n">targets</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(0.2333)
</pre></div>
</div>
</div>
</div>
</section>
<section id="ensuring-predictions-are-between-0-and-1-with-the-sigmoid-function">
<h3>Ensuring Predictions are Between 0 and 1 with the Sigmoid Function<a class="headerlink" href="#ensuring-predictions-are-between-0-and-1-with-the-sigmoid-function" title="Link to this heading">#</a></h3>
<p>To use the loss function effectively, we need to ensure that predictions fall between 0 and 1. This can be done using the <strong>sigmoid function</strong>, defined as:</p>
<div class="math notranslate nohighlight">
\[ \text{sigmoid}(x) = \frac{1}{1 + e^{-x}} \]</div>
<p>The sigmoid function squashes any input to a range between 0 and 1, making it perfect for binary classification tasks. In PyTorch, we can use <code class="docutils literal notranslate"><span class="pre">torch.sigmoid</span></code> for this purpose.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the sigmoid function (PyTorch version)</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Plot the sigmoid function</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sigmoid Function&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Input&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Output (0 to 1)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6594b01f356185bbe55c7393d08048b00f517296481d0cdf4be7c74d1a037f44.png" src="../_images/6594b01f356185bbe55c7393d08048b00f517296481d0cdf4be7c74d1a037f44.png" />
</div>
</div>
<p>The sigmoid function maps any real-valued input to a value between 0 and 1, which means that predictions can now represent probabilities.</p>
</section>
<section id="updating-the-loss-function-with-sigmoid">
<h3>Updating the Loss Function with Sigmoid<a class="headerlink" href="#updating-the-loss-function-with-sigmoid" title="Link to this heading">#</a></h3>
<p>We can update our <code class="docutils literal notranslate"><span class="pre">mnist_loss</span></code> function to first apply the sigmoid to the predictions. This ensures that the loss function works even if the model outputs are not constrained between 0 and 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Updated MNIST loss function with sigmoid</span>
<span class="k">def</span> <span class="nf">mnist_loss</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="why-do-we-need-a-separate-loss-function">
<h3>Why Do We Need a Separate Loss Function?<a class="headerlink" href="#why-do-we-need-a-separate-loss-function" title="Link to this heading">#</a></h3>
<p>The key difference between <strong>metrics</strong> (like accuracy) and <strong>loss functions</strong> is their purpose:</p>
<ul class="simple">
<li><p><strong>Metrics</strong>: Used to evaluate model performance, providing a number that indicates how well the model performs on a specific task. Metrics don’t need to be differentiable, as they’re not used in the optimization process.</p></li>
<li><p><strong>Loss Functions</strong>: Guide the learning process by calculating gradients, which indicate how to adjust model parameters to improve performance. Loss functions must be smooth and differentiable, as their derivatives are used in optimization.</p></li>
</ul>
<p>By using this custom loss function, we provide a signal that tells the model to get closer to correct answers, even when predictions are slightly off.</p>
</section>
</section>
<section id="optimization-with-stochastic-gradient-descent-sgd-and-mini-batches">
<h2>Optimization with Stochastic Gradient Descent (SGD) and Mini-Batches<a class="headerlink" href="#optimization-with-stochastic-gradient-descent-sgd-and-mini-batches" title="Link to this heading">#</a></h2>
<p>In machine learning, <strong>Stochastic Gradient Descent (SGD)</strong> is an optimization technique used to minimize the loss function, which guides the learning process of a model. Unlike regular <strong>Gradient Descent</strong>, which computes the gradient of the loss with respect to each parameter over the entire dataset, SGD approximates this by computing gradients over small, random subsets of the data called <strong>mini-batches</strong>.</p>
<p>SGD allows for faster training and can lead to better generalization due to the randomness in mini-batch sampling, which helps the model avoid getting stuck in certain local minima.</p>
<section id="why-use-mini-batches">
<h3>Why Use Mini-Batches?<a class="headerlink" href="#why-use-mini-batches" title="Link to this heading">#</a></h3>
<p>Using mini-batches instead of the whole dataset or a single data item for each gradient update has several advantages:</p>
<ol class="arabic simple">
<li><p><strong>Faster Computation</strong>: Calculating gradients over the entire dataset can be slow, especially for large datasets. Mini-batches allow us to update model parameters more frequently without waiting to process the whole dataset.</p></li>
<li><p><strong>Stable Gradient Estimates</strong>: A single data item may produce a noisy and unstable gradient estimate, while the entire dataset provides a more accurate but computationally expensive estimate. Mini-batches provide a balance between these two, resulting in more stable gradient updates.</p></li>
<li><p><strong>Optimized GPU Utilization</strong>: When using hardware accelerators like GPUs, mini-batches allow for parallel computations, maximizing the utilization of these resources.</p></li>
<li><p><strong>Improved Generalization</strong>: The random sampling of mini-batches introduces stochasticity (randomness) in parameter updates, helping the model avoid overfitting and improve generalization.</p></li>
</ol>
</section>
<section id="choosing-a-batch-size">
<h3>Choosing a Batch Size<a class="headerlink" href="#choosing-a-batch-size" title="Link to this heading">#</a></h3>
<p>The number of data points in a mini-batch is called the <strong>batch size</strong>. The choice of batch size affects both the training time and the model’s performance:</p>
<ul class="simple">
<li><p><strong>Larger batch sizes</strong> produce more stable gradient estimates, but training may be slower per update step.</p></li>
<li><p><strong>Smaller batch sizes</strong> allow for faster updates but may produce noisy gradients, which can make training less stable.</p></li>
</ul>
</section>
<section id="shuffling-the-data">
<h3>Shuffling the Data<a class="headerlink" href="#shuffling-the-data" title="Link to this heading">#</a></h3>
<p>To prevent the model from seeing data in a fixed order, which could introduce unwanted patterns in updates, it’s standard practice to shuffle the dataset before dividing it into mini-batches. Shuffling the data in each epoch ensures that the model doesn’t rely on any specific order in the data.</p>
<p>PyTorch provides the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> class, which automatically handles shuffling and mini-batch creation. Here’s a simple example of using <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> to create mini-batches from a dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># Sample data for demonstration</span>
<span class="n">data</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Create a DataLoader with batch size 5 and shuffling enabled</span>
<span class="n">dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">list</span><span class="p">(</span><span class="n">dl</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[tensor([10,  8, 15, 16, 17]),
 tensor([ 6, 12, 19, 18,  5]),
 tensor([ 4,  1, 13,  9, 11]),
 tensor([ 3, 14,  7,  0,  2])]
</pre></div>
</div>
</div>
</div>
<p>In this example, <code class="docutils literal notranslate"><span class="pre">dl</span></code> creates batches of size 5, and the <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code> argument ensures that the data order is randomized with each epoch.</p>
</section>
<section id="applying-mini-batches-to-images-and-labels">
<h3>Applying Mini-Batches to Images and Labels<a class="headerlink" href="#applying-mini-batches-to-images-and-labels" title="Link to this heading">#</a></h3>
<p>For our MNIST classification task, we need to process both the images (features) and the labels (targets). Here’s how we can create a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> for the MNIST dataset:</p>
<ol class="arabic simple">
<li><p><strong>Combine images and labels</strong> into a PyTorch Dataset.</p></li>
<li><p><strong>Use a DataLoader</strong> to create mini-batches of this dataset.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create the dataset for training</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">stacked_threes</span><span class="p">,</span> <span class="n">stacked_sevens</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">stacked_threes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">stacked_sevens</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">))</span>
<span class="n">valid_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">valid_3_tens</span><span class="p">,</span> <span class="n">valid_7_tens</span><span class="p">])</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">)</span>
<span class="n">valid_y</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_3_tens</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_7_tens</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">valid_dset</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">valid_x</span><span class="p">,</span><span class="n">valid_y</span><span class="p">))</span>

<span class="c1"># Create the DataLoader for mini-batch processing</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> here will yield mini-batches of 256 images and their corresponding labels, shuffling the dataset in each epoch.</p>
</section>
<section id="implementing-an-sgd-step-with-mini-batches">
<h3>Implementing an SGD Step with Mini-Batches<a class="headerlink" href="#implementing-an-sgd-step-with-mini-batches" title="Link to this heading">#</a></h3>
<p>For each mini-batch, we need to:</p>
<ol class="arabic simple">
<li><p><strong>Calculate predictions</strong> based on the current model parameters.</p></li>
<li><p><strong>Compute the loss</strong> to see how far these predictions are from the true labels.</p></li>
<li><p><strong>Calculate gradients</strong> with respect to each parameter.</p></li>
<li><p><strong>Update parameters</strong> by subtracting a small proportion (learning rate) of the gradient from each parameter.</p></li>
</ol>
<p>Here’s an implementation of one epoch of SGD with mini-batches:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the learning rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Training loop for one epoch</span>
<span class="k">def</span> <span class="nf">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="c1"># Forward pass: calculate predictions</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
        
        <span class="c1"># Calculate loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">mnist_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
        
        <span class="c1"># Backward pass: compute gradients</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="c1"># Update parameters and zero gradients</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="updating-parameters-with-optimizer-step">
<h3>Updating Parameters with <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code><a class="headerlink" href="#updating-parameters-with-optimizer-step" title="Link to this heading">#</a></h3>
<p>The line <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> updates the parameters using the computed gradients. We then use <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> to reset gradients before the next mini-batch; otherwise, PyTorch accumulates gradients, which would lead to incorrect updates.</p>
</section>
<section id="mini-batch-accuracy-calculation">
<h3>Mini-Batch Accuracy Calculation<a class="headerlink" href="#mini-batch-accuracy-calculation" title="Link to this heading">#</a></h3>
<p>To evaluate the model’s performance, we can compute the accuracy on the validation set. For each mini-batch, we check if each prediction matches the true label and average these correct predictions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define accuracy calculation function</span>
<span class="k">def</span> <span class="nf">batch_accuracy</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">xb</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">preds</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">==</span> <span class="n">yb</span>
    <span class="k">return</span> <span class="n">correct</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Validation accuracy calculation</span>
<span class="k">def</span> <span class="nf">validate_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch_accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">),</span> <span class="n">yb</span><span class="p">)</span> <span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">accs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="putting-it-all-together">
<h3>Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Link to this heading">#</a></h3>
<p>We can now use these functions to train our model over several epochs, updating the parameters based on each mini-batch and calculating the accuracy on the validation set at the end of each epoch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the number of epochs</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Initialize model and optimizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Simple linear model for demonstration</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">train_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> accuracy:&quot;</span><span class="p">,</span> <span class="n">validate_epoch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1 accuracy: 0.9653
Epoch 2 accuracy: 0.9662
Epoch 3 accuracy: 0.9677
Epoch 4 accuracy: 0.9692
Epoch 5 accuracy: 0.9696
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="creating-an-optimizer">
<h2>Creating an Optimizer<a class="headerlink" href="#creating-an-optimizer" title="Link to this heading">#</a></h2>
<p>An <strong>optimizer</strong> is an essential component in training a neural network, responsible for updating the model’s parameters (weights and biases) based on the computed gradients. This process is typically done after each mini-batch, where the optimizer adjusts the parameters in the direction that minimizes the loss function.</p>
<section id="why-do-we-need-an-optimizer">
<h3>Why Do We Need an Optimizer?<a class="headerlink" href="#why-do-we-need-an-optimizer" title="Link to this heading">#</a></h3>
<p>In our previous section, we implemented Stochastic Gradient Descent (SGD) manually by:</p>
<ol class="arabic simple">
<li><p>Calculating gradients for each parameter using backpropagation.</p></li>
<li><p>Subtracting a fraction of the gradient (scaled by the learning rate) from each parameter.</p></li>
</ol>
<p>While this process works, it can be simplified and optimized further using dedicated optimizers. PyTorch provides several built-in optimizers, each with different strategies for updating parameters. Using these optimizers streamlines the code and allows us to experiment with different optimization techniques easily.</p>
</section>
<section id="common-optimizers-in-deep-learning">
<h3>Common Optimizers in Deep Learning<a class="headerlink" href="#common-optimizers-in-deep-learning" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>SGD (Stochastic Gradient Descent)</strong>: Updates parameters by subtracting a fraction of the gradient. This method is simple but can be slow and may oscillate around local minima.</p></li>
<li><p><strong>Momentum-based SGD</strong>: Builds on SGD by accumulating gradients in the direction of steepest descent, which helps accelerate convergence.</p></li>
<li><p><strong>Adam (Adaptive Moment Estimation)</strong>: Uses adaptive learning rates for each parameter, combining the ideas of momentum and adaptive learning rates, making it popular for many deep learning applications.</p></li>
</ol>
<p>For this section, we’ll start by creating a basic custom optimizer similar to SGD, then show how to use PyTorch’s built-in optimizers.</p>
</section>
<section id="custom-basic-optimizer">
<h3>Custom Basic Optimizer<a class="headerlink" href="#custom-basic-optimizer" title="Link to this heading">#</a></h3>
<p>Let’s define a simple optimizer called <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> that implements standard SGD. This optimizer will:</p>
<ol class="arabic simple">
<li><p>Store the parameters to be updated.</p></li>
<li><p>Take a step size (learning rate) to scale the updates.</p></li>
<li><p>Include a <code class="docutils literal notranslate"><span class="pre">step</span></code> method to update each parameter by subtracting the product of the gradient and learning rate.</p></li>
<li><p>Reset gradients after each update to avoid accumulating them.</p></li>
</ol>
<p>Here’s an implementation of a basic optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a custom basic optimizer</span>
<span class="k">class</span> <span class="nc">BasicOptim</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>  <span class="c1"># Parameters to optimize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>                <span class="c1"># Learning rate</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Update each parameter by its gradient</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Set gradients to None for each parameter</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">:</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-basicoptim-with-a-model">
<h3>Using <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> with a Model<a class="headerlink" href="#using-basicoptim-with-a-model" title="Link to this heading">#</a></h3>
<p>To use this optimizer, we pass the model’s parameters to <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> and specify a learning rate. Here’s a simple training loop that demonstrates how to use this custom optimizer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize model and optimizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Simple linear model for demonstration</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">BasicOptim</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Training loop for one epoch</span>
<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="c1"># Forward pass: calculate predictions</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    
    <span class="c1"># Calculate loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mnist_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    
    <span class="c1"># Backward pass: compute gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    
    <span class="c1"># Update parameters and zero gradients</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>In this example:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">opt.step()</span></code> updates each parameter by subtracting the scaled gradient.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">opt.zero_grad()</span></code> resets gradients for each parameter, ensuring that gradients from one mini-batch don’t interfere with the next.</p></li>
</ul>
</section>
<section id="using-pytorchs-built-in-optimizers">
<h3>Using PyTorch’s Built-in Optimizers<a class="headerlink" href="#using-pytorchs-built-in-optimizers" title="Link to this heading">#</a></h3>
<p>While <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> demonstrates the core concept of SGD, PyTorch provides a built-in <code class="docutils literal notranslate"><span class="pre">SGD</span></code> optimizer, which includes additional features like momentum. Using PyTorch’s <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code> library simplifies optimization and provides more options.</p>
<p>Here’s an example of using PyTorch’s built-in <code class="docutils literal notranslate"><span class="pre">SGD</span></code> optimizer with momentum:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># Initialize model and optimizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mnist_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>With PyTorch’s <code class="docutils literal notranslate"><span class="pre">SGD</span></code> optimizer, we only need to specify the learning rate and optionally, a momentum factor. The optimizer handles both the parameter updates and gradient reset internally.</p>
</section>
<section id="switching-to-the-adam-optimizer">
<h3>Switching to the Adam Optimizer<a class="headerlink" href="#switching-to-the-adam-optimizer" title="Link to this heading">#</a></h3>
<p>Adam (Adaptive Moment Estimation) is another popular optimizer that automatically adjusts learning rates for each parameter based on historical gradients and updates. This approach often improves convergence, especially in deep networks.</p>
<p>Switching to Adam requires only a change in the optimizer initialization, while the training loop remains the same:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="c1"># Initialize model and optimizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>  <span class="c1"># Lower learning rate for Adam</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">mnist_loss</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>With Adam, you typically use a lower learning rate (e.g., <code class="docutils literal notranslate"><span class="pre">0.001</span></code>) than with SGD, as Adam adapts the learning rate dynamically during training.</p>
</section>
<section id="using-fastais-learner-with-optimizers">
<h3>Using fastai’s <code class="docutils literal notranslate"><span class="pre">Learner</span></code> with Optimizers<a class="headerlink" href="#using-fastais-learner-with-optimizers" title="Link to this heading">#</a></h3>
<p>The fastai library simplifies training by integrating the optimizer, loss function, and metrics within a single class, the <code class="docutils literal notranslate"><span class="pre">Learner</span></code>. To use <code class="docutils literal notranslate"><span class="pre">Learner</span></code>, we need to:</p>
<ol class="arabic simple">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">DataLoaders</span></code> object from our training and validation data.</p></li>
<li><p>Pass the model, optimizer, loss function, and metrics to <code class="docutils literal notranslate"><span class="pre">Learner</span></code>.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="n">Learner</span><span class="p">,</span> <span class="n">DataLoaders</span><span class="p">,</span> <span class="n">Adam</span> 

<span class="c1"># Create DataLoaders</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">DataLoaders</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>

<span class="c1"># Initialize Learner with model, optimizer, and metrics</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">Adam</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">mnist_loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">batch_accuracy</span><span class="p">)</span>

<span class="c1"># Train model using fit</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.116063</td>
      <td>0.087859</td>
      <td>0.965653</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.082810</td>
      <td>0.064342</td>
      <td>0.969578</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.064231</td>
      <td>0.053253</td>
      <td>0.970559</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.051999</td>
      <td>0.046439</td>
      <td>0.972031</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.044355</td>
      <td>0.041906</td>
      <td>0.973994</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.038709</td>
      <td>0.038427</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.034562</td>
      <td>0.036027</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.031878</td>
      <td>0.033544</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.028985</td>
      <td>0.031644</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.027182</td>
      <td>0.030063</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table></div></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">Learner</span></code>, we can use the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method to train our model over multiple epochs, making it easier to manage training while automatically handling optimizations.</p>
</section>
</section>
<section id="adding-a-nonlinearity">
<h2>Adding a Nonlinearity<a class="headerlink" href="#adding-a-nonlinearity" title="Link to this heading">#</a></h2>
<p>So far, we have only used linear transformations, which are limited in their capacity to capture complex patterns. Adding a <strong>nonlinear activation function</strong> like the <em>ReLU</em> (Rectified Linear Unit) between layers gives our model the ability to approximate more complex functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># Define a simple neural network</span>
<span class="n">simple_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">simple_net</span><span class="p">,</span> <span class="n">opt_func</span><span class="o">=</span><span class="n">Adam</span><span class="p">,</span>
                <span class="n">loss_func</span><span class="o">=</span><span class="n">mnist_loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">batch_accuracy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#hide_output</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"><table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>batch_accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.362022</td>
      <td>0.312601</td>
      <td>0.837586</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.314636</td>
      <td>0.289494</td>
      <td>0.902846</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.295409</td>
      <td>0.281034</td>
      <td>0.921982</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.284432</td>
      <td>0.276199</td>
      <td>0.934249</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.277537</td>
      <td>0.273129</td>
      <td>0.942100</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>5</td>
      <td>0.273161</td>
      <td>0.270878</td>
      <td>0.948970</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.270442</td>
      <td>0.269329</td>
      <td>0.951423</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.269564</td>
      <td>0.268082</td>
      <td>0.953876</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>8</td>
      <td>0.266938</td>
      <td>0.267078</td>
      <td>0.957311</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>9</td>
      <td>0.266108</td>
      <td>0.266248</td>
      <td>0.960255</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>10</td>
      <td>0.265036</td>
      <td>0.265703</td>
      <td>0.961236</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>11</td>
      <td>0.264270</td>
      <td>0.265083</td>
      <td>0.965162</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>12</td>
      <td>0.263208</td>
      <td>0.264801</td>
      <td>0.964671</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>13</td>
      <td>0.262102</td>
      <td>0.264268</td>
      <td>0.967125</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>14</td>
      <td>0.262263</td>
      <td>0.263753</td>
      <td>0.968106</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>15</td>
      <td>0.261472</td>
      <td>0.263396</td>
      <td>0.969087</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>16</td>
      <td>0.261426</td>
      <td>0.263057</td>
      <td>0.970559</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>17</td>
      <td>0.262339</td>
      <td>0.262738</td>
      <td>0.973013</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>18</td>
      <td>0.260858</td>
      <td>0.262552</td>
      <td>0.973503</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>19</td>
      <td>0.259854</td>
      <td>0.262473</td>
      <td>0.973013</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>20</td>
      <td>0.259995</td>
      <td>0.262366</td>
      <td>0.972522</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>21</td>
      <td>0.259688</td>
      <td>0.262176</td>
      <td>0.973503</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>22</td>
      <td>0.259385</td>
      <td>0.262138</td>
      <td>0.973013</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>23</td>
      <td>0.259779</td>
      <td>0.261703</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>24</td>
      <td>0.259479</td>
      <td>0.261862</td>
      <td>0.974485</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>25</td>
      <td>0.259223</td>
      <td>0.261361</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>26</td>
      <td>0.260129</td>
      <td>0.261441</td>
      <td>0.975957</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>27</td>
      <td>0.259477</td>
      <td>0.261181</td>
      <td>0.976448</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>28</td>
      <td>0.259526</td>
      <td>0.261378</td>
      <td>0.975466</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>29</td>
      <td>0.259272</td>
      <td>0.261068</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.258793</td>
      <td>0.261166</td>
      <td>0.975957</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>31</td>
      <td>0.259357</td>
      <td>0.260903</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>32</td>
      <td>0.258865</td>
      <td>0.260969</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>33</td>
      <td>0.258719</td>
      <td>0.260658</td>
      <td>0.981354</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>34</td>
      <td>0.258948</td>
      <td>0.260875</td>
      <td>0.976448</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>35</td>
      <td>0.257936</td>
      <td>0.260849</td>
      <td>0.976938</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>36</td>
      <td>0.258122</td>
      <td>0.260750</td>
      <td>0.977429</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>37</td>
      <td>0.258659</td>
      <td>0.260602</td>
      <td>0.980373</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>38</td>
      <td>0.258248</td>
      <td>0.260937</td>
      <td>0.975957</td>
      <td>00:00</td>
    </tr>
    <tr>
      <td>39</td>
      <td>0.258983</td>
      <td>0.260659</td>
      <td>0.978410</td>
      <td>00:00</td>
    </tr>
  </tbody>
</table></div></div>
</div>
<p>The training process is recorded in <code class="docutils literal notranslate"><span class="pre">learn.recorder</span></code>, with the table of output stored in the <code class="docutils literal notranslate"><span class="pre">values</span></code> attribute, so we can plot the accuracy over training as:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">L</span><span class="p">(</span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">itemgot</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">scaley</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/27c1795c8116a57a954b75ea726903a5256bd26b055e3349aaa31d69d318e8b3.png" src="../_images/27c1795c8116a57a954b75ea726903a5256bd26b055e3349aaa31d69d318e8b3.png" />
</div>
</div>
<p>And we can view the final accuracy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">recorder</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9784101843833923
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="23_NN_as_UA.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Neural Networks as Universal Approximators</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixels-the-foundations-of-computer-vision">Pixels: The Foundations of Computer Vision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pixel-similarity">Pixel Similarity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-metrics-using-broadcasting">Computing Metrics Using Broadcasting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-tensors-and-broadcasting">PyTorch Tensors and Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-distance-function-with-broadcasting">Defining a Distance Function with Broadcasting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classifying-an-image-with-distance">Classifying an Image with Distance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-accuracy">Calculating Accuracy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-loss-function">Defining a Loss Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-use-accuracy-as-a-loss-function">Why Not Use Accuracy as a Loss Function?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#defining-a-loss-function-for-binary-classification">Defining a Loss Function for Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-a-simple-loss-function">Implementing a Simple Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ensuring-predictions-are-between-0-and-1-with-the-sigmoid-function">Ensuring Predictions are Between 0 and 1 with the Sigmoid Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-the-loss-function-with-sigmoid">Updating the Loss Function with Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-a-separate-loss-function">Why Do We Need a Separate Loss Function?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-with-stochastic-gradient-descent-sgd-and-mini-batches">Optimization with Stochastic Gradient Descent (SGD) and Mini-Batches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-mini-batches">Why Use Mini-Batches?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-batch-size">Choosing a Batch Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-the-data">Shuffling the Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-mini-batches-to-images-and-labels">Applying Mini-Batches to Images and Labels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-an-sgd-step-with-mini-batches">Implementing an SGD Step with Mini-Batches</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updating-parameters-with-optimizer-step">Updating Parameters with <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-accuracy-calculation">Mini-Batch Accuracy Calculation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-an-optimizer">Creating an Optimizer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-an-optimizer">Why Do We Need an Optimizer?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-optimizers-in-deep-learning">Common Optimizers in Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#custom-basic-optimizer">Custom Basic Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-basicoptim-with-a-model">Using <code class="docutils literal notranslate"><span class="pre">BasicOptim</span></code> with a Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorchs-built-in-optimizers">Using PyTorch’s Built-in Optimizers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#switching-to-the-adam-optimizer">Switching to the Adam Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-fastais-learner-with-optimizers">Using fastai’s <code class="docutils literal notranslate"><span class="pre">Learner</span></code> with Optimizers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-a-nonlinearity">Adding a Nonlinearity</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Biagio Mandracchia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>