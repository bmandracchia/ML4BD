
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Classification with Multilayer Perceptrons &#8212; Introduction to Machine Learning for Biomedical Data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/24_MLP';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Convolutional Neural Networks" href="25_convolutions.html" />
    <link rel="prev" title="Training a Digit Classifier" href="23_Loss_Metrics_and_Optimizers.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Introduction to Machine Learning for Biomedical Data - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Introduction to Machine Learning for Biomedical Data - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Computational Tools</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_python_primer.html">Python: a Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_data_science_with_pandas.html">Data Science with Python and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_jupyter_markdown.html">Jupyter &amp; Markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Feature Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="11_machine_learning_fundamentals.html">The Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="12_basic_feature_engineering.html">Basic Feature Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="13_categorical_variables.html">Categorical Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_classifiers.html">Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_dimensionality_reduction.html">Dimensionality Reduction: PCA</a></li>
<li class="toctree-l1"><a class="reference internal" href="17_kmeans.html">Nonlinear Manifold Feature Extraction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="21_neural%20networks.html">Deep Learning in Biomedical Engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="22_NN_as_UA.html">Neural Networks as Universal Approximators</a></li>
<li class="toctree-l1"><a class="reference internal" href="23_Loss_Metrics_and_Optimizers.html">Training a Digit Classifier</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Classification with Multilayer Perceptrons</a></li>
<li class="toctree-l1"><a class="reference internal" href="25_convolutions.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="26_resnet2.html">ResNets for Biomedical Image Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="28_probabilistic.html">Introduction to Probabilistic Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="29_PBDL.html">Introdution to Physics-Based Deep Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/bmandracchia/ML4BD/issues/new?title=Issue%20on%20page%20%2Fchapters/24_MLP.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/24_MLP.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification with Multilayer Perceptrons</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-multilayer-perceptrons">Implementation of Multilayer Perceptrons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-model-parameters">Initializing Model Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-datablock">Building the DataBlock</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-datablock-parameters">Explanation of the <code class="docutils literal notranslate"><span class="pre">DataBlock</span></code> Parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validating-the-data-pipeline">Validating the Data Pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">Training the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cross-entropy-loss">The Cross-Entropy Loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-model">Interpreting the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-the-model">Improving the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate">The Learning Rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-finder">Learning Rate Finder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-optimization-selecting-epochs-and-avoiding-overfitting">Further Optimization: Selecting Epochs and Avoiding Overfitting</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="classification-with-multilayer-perceptrons">
<h1>Classification with Multilayer Perceptrons<a class="headerlink" href="#classification-with-multilayer-perceptrons" title="Link to this heading">#</a></h1>
<p>Here, we will introduce the simplest deep networks, which are called <strong>multilayer perceptrons (MLP)</strong>, and they consist of multiple layers of neurons each fully connected to those in the layer below (from which they receive input) and those above (which they, in turn, influence).</p>
<p>MLP overcome the limitations of linear models by incorporating one or more hidden layers. The easiest way to do this is to stack many fully connected layers on top of one another. Each layer feeds into the layer above it, until we generate outputs. We can think of the first
layers as our representation and the final layer as our linear predictor.</p>
<p><img alt="image.png" src="chapters/attachment:image.png" /></p>
<section id="implementation-of-multilayer-perceptrons">
<h2>Implementation of Multilayer Perceptrons<a class="headerlink" href="#implementation-of-multilayer-perceptrons" title="Link to this heading">#</a></h2>
<p>Multilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. The key conceptual difference is that we now concatenate multiple layers.</p>
<section id="initializing-model-parameters">
<h3>Initializing Model Parameters<a class="headerlink" href="#initializing-model-parameters" title="Link to this heading">#</a></h3>
<p>The MNIST dataset contains 10 classes, and that each image consists of a <span class="math notranslate nohighlight">\( 28 \times 28 = 784\)</span> grid of grayscale pixel values. As before we will disregard the spatial structure among the pixels for now, so we can think of this as a classification dataset with 784 input features and 10 classes.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">MNIST</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>To begin, we will implement an MLP with one hidden layer and 256 hidden units. Both the number of layers and their width are adjustable (they are considered hyperparameters). Typically, we choose the layer widths to be divisible by larger powers of 2. This is computationally efficient due to the way memory is allocated and addressed in hardware.</p>
<p>Again, we will represent our parameters with several tensors. Note that for every layer, we must keep track of one weight matrix and one bias vector. As always, we allocate memory for the gradients of the loss with respect to these parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_hiddens</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">num_outputs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_hiddens</span><span class="p">),</span>
                                 <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">LazyLinear</span><span class="p">(</span><span class="n">num_outputs</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="building-the-datablock">
<h3>Building the DataBlock<a class="headerlink" href="#building-the-datablock" title="Link to this heading">#</a></h3>
<p>With our extraction pattern in place, we’ll use fastai’s <code class="docutils literal notranslate"><span class="pre">DataBlock</span></code> API to set up the data pipeline. <code class="docutils literal notranslate"><span class="pre">DataBlock</span></code> helps you organize and transform data for training, providing a flexible approach to defining inputs, outputs, and transformations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dgt</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>
    <span class="n">get_items</span><span class="o">=</span><span class="n">get_image_files</span><span class="p">,</span>
    <span class="n">get_y</span><span class="o">=</span><span class="n">parent_label</span><span class="p">,</span>
    <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">batch_tfms</span><span class="o">=</span><span class="n">aug_transforms</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">dgt</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="explanation-of-the-datablock-parameters">
<h4>Explanation of the <code class="docutils literal notranslate"><span class="pre">DataBlock</span></code> Parameters<a class="headerlink" href="#explanation-of-the-datablock-parameters" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">blocks=(ImageBlock,</span> <span class="pre">CategoryBlock)</span></code></strong>: Specifies that the input type is an image and the output is a category (label).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">get_items=get_image_files</span></code></strong>: Retrieves all image files within the specified path.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">get_y=parent_label</span></code></strong>: Retrieves the label for each image from the name of the parent folder.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">splitter=RandomSplitter(seed=42)</span></code></strong>: Randomly splits the dataset into training and validation sets. Setting a seed ensures consistency across runs.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">batch_tfms=aug_transforms(size=224,</span> <span class="pre">min_scale=0.75)</span></code></strong>: Performs batch-level transformations, such as applying data augmentation.</p></li>
</ul>
</section>
</section>
</section>
<section id="validating-the-data-pipeline">
<h2>Validating the Data Pipeline<a class="headerlink" href="#validating-the-data-pipeline" title="Link to this heading">#</a></h2>
<p>Before training, always verify the data pipeline. Errors or misaligned labels can lead to incorrect model training. We can check the data layout using <code class="docutils literal notranslate"><span class="pre">show_batch</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7c37077ce034dd0e5c9c931b5d8ec6262acac8ae7a92335a96cfc8ef318d7c46.png" src="../_images/7c37077ce034dd0e5c9c931b5d8ec6262acac8ae7a92335a96cfc8ef318d7c46.png" />
</div>
</div>
<p>This visualizes a batch of images and their labels, helping confirm that images match expected labels. If needed, use the <code class="docutils literal notranslate"><span class="pre">summary</span></code> method to display a detailed breakdown of data transformations and steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dgt</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Setting-up type transforms pipelines
Collecting items from C:\Users\biagi\.fastai\data\mnist_png
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 70000 items
2 datasets of sizes 56000,14000
Setting up Pipeline: PILBase.create
Setting up Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False}

Building one sample
  Pipeline: PILBase.create
    starting from
      C:\Users\biagi\.fastai\data\mnist_png\training\7\48923.png
    applying PILBase.create gives
      PILImage mode=RGB size=28x28
  Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False}
    starting from
      C:\Users\biagi\.fastai\data\mnist_png\training\7\48923.png
    applying parent_label gives
      7
    applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives
      TensorCategory(7)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final sample: (PILImage mode=RGB size=28x28, TensorCategory(7))


Collecting items from C:\Users\biagi\.fastai\data\mnist_png
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Found 70000 items
2 datasets of sizes 56000,14000
Setting up Pipeline: PILBase.create
Setting up Pipeline: parent_label -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False}
Setting up after_item: Pipeline: ToTensor
Setting up before_batch: Pipeline: 
Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False}

Building one batch
Applying item_tfms to the first sample:
  Pipeline: ToTensor
    starting from
      (PILImage mode=RGB size=28x28, TensorCategory(7))
    applying ToTensor gives
      (TensorImage of size 3x28x28, TensorCategory(7))

Adding the next 3 samples
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No before_batch transform to apply

Collating items in a batch

Applying batch_tfms to the batch built
  Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False}
    starting from
      (TensorImage of size 4x3x28x28, TensorCategory([7, 7, 2, 2]))
    applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives
      (TensorImage of size 4x3x28x28, TensorCategory([7, 7, 2, 2]))
    applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives
      (TensorImage of size 4x3x28x28, TensorCategory([7, 7, 2, 2]))
    applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives
      (TensorImage of size 4x3x28x28, TensorCategory([7, 7, 2, 2]))
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-the-model">
<h2>Training the Model<a class="headerlink" href="#training-the-model" title="Link to this heading">#</a></h2>
<p>Now that our data is ready, we can proceed to training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html">
    <div>
      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/2 00:00&lt;?]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>error_rate</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>

    <div>
      <progress value='330' class='' max='875' style='width:300px; height:20px; vertical-align: middle;'></progress>
      37.71% [330/875 00:29&lt;00:49 0.7159]
    </div>
    </div><div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">line</span> <span class="mi">2</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="n">learn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:266,</span> in <span class="ni">Learner.fit</span><span class="nt">(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)</span>
<span class="g g-Whitespace">    </span><span class="mi">264</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">set_hypers</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">lr</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">265</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_epoch</span> <span class="o">=</span> <span class="n">n_epoch</span>
<span class="ne">--&gt; </span><span class="mi">266</span> <span class="bp">self</span><span class="o">.</span><span class="n">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_do_fit</span><span class="p">,</span> <span class="s1">&#39;fit&#39;</span><span class="p">,</span> <span class="n">CancelFitException</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_end_cleanup</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:201,</span> in <span class="ni">Learner._with_events</span><span class="nt">(self, f, event_type, ex, final)</span>
<span class="g g-Whitespace">    </span><span class="mi">200</span> <span class="k">def</span> <span class="nf">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">event_type</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">final</span><span class="o">=</span><span class="n">noop</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">201</span>     <span class="k">try</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;before_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">f</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">202</span>     <span class="k">except</span> <span class="n">ex</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_cancel_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>     <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">final</span><span class="p">()</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:255,</span> in <span class="ni">Learner._do_fit</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">253</span> <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_epoch</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>     <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span>
<span class="ne">--&gt; </span><span class="mi">255</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_do_epoch</span><span class="p">,</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="n">CancelEpochException</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:201,</span> in <span class="ni">Learner._with_events</span><span class="nt">(self, f, event_type, ex, final)</span>
<span class="g g-Whitespace">    </span><span class="mi">200</span> <span class="k">def</span> <span class="nf">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">event_type</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">final</span><span class="o">=</span><span class="n">noop</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">201</span>     <span class="k">try</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;before_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">f</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">202</span>     <span class="k">except</span> <span class="n">ex</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_cancel_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>     <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">final</span><span class="p">()</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:249,</span> in <span class="ni">Learner._do_epoch</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span> <span class="k">def</span> <span class="nf">_do_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">249</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_do_epoch_train</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_do_epoch_validate</span><span class="p">()</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:241,</span> in <span class="ni">Learner._do_epoch_train</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">239</span> <span class="k">def</span> <span class="nf">_do_epoch_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">240</span>     <span class="bp">self</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">train</span>
<span class="ne">--&gt; </span><span class="mi">241</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">all_batches</span><span class="p">,</span> <span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">CancelTrainException</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:201,</span> in <span class="ni">Learner._with_events</span><span class="nt">(self, f, event_type, ex, final)</span>
<span class="g g-Whitespace">    </span><span class="mi">200</span> <span class="k">def</span> <span class="nf">_with_events</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">event_type</span><span class="p">,</span> <span class="n">ex</span><span class="p">,</span> <span class="n">final</span><span class="o">=</span><span class="n">noop</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">201</span>     <span class="k">try</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;before_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">f</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">202</span>     <span class="k">except</span> <span class="n">ex</span><span class="p">:</span> <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_cancel_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">203</span>     <span class="bp">self</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;after_</span><span class="si">{</span><span class="n">event_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">);</span>  <span class="n">final</span><span class="p">()</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:207,</span> in <span class="ni">Learner.all_batches</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">205</span> <span class="k">def</span> <span class="nf">all_batches</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">206</span>     <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dl</span><span class="p">)</span>
<span class="nn">--&gt; 207     for o</span> in <span class="ni">enumerate(self.dl): self.one_batch</span><span class="nt">(*o)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\load.py:129,</span> in <span class="ni">DataLoader.__iter__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">127</span> <span class="bp">self</span><span class="o">.</span><span class="n">before_iter</span><span class="p">()</span>
<span class="nn">    128 self.__idxs=self.get_idxs() # called</span> in <span class="ni">context of main process </span><span class="nt">(not workers/subprocesses)</span>
<span class="ne">--&gt; </span><span class="mi">129</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">_loaders</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">fake_l</span><span class="o">.</span><span class="n">num_workers</span><span class="o">==</span><span class="mi">0</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">fake_l</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">130</span>     <span class="c1"># pin_memory causes tuples to be converted to lists, so convert them back to tuples</span>
<span class="g g-Whitespace">    </span><span class="mi">131</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span> <span class="n">b</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">132</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">b</span> <span class="o">=</span> <span class="n">to_device</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\torch\utils\data\dataloader.py:630,</span> in <span class="ni">_BaseDataLoaderIter.__next__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">627</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler_iter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">628</span>     <span class="c1"># TODO(https://github.com/pytorch/pytorch/issues/76750)</span>
<span class="g g-Whitespace">    </span><span class="mi">629</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_reset</span><span class="p">()</span>  <span class="c1"># type: ignore[call-arg]</span>
<span class="ne">--&gt; </span><span class="mi">630</span> <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_data</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">631</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="g g-Whitespace">    </span><span class="mi">632</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_kind</span> <span class="o">==</span> <span class="n">_DatasetKind</span><span class="o">.</span><span class="n">Iterable</span> <span class="ow">and</span> \
<span class="g g-Whitespace">    </span><span class="mi">633</span>         <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> \
<span class="g g-Whitespace">    </span><span class="mi">634</span>         <span class="bp">self</span><span class="o">.</span><span class="n">_num_yielded</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_IterableDataset_len_called</span><span class="p">:</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\torch\utils\data\dataloader.py:673,</span> in <span class="ni">_SingleProcessDataLoaderIter._next_data</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">671</span> <span class="k">def</span> <span class="nf">_next_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">672</span>     <span class="n">index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_next_index</span><span class="p">()</span>  <span class="c1"># may raise StopIteration</span>
<span class="ne">--&gt; </span><span class="mi">673</span>     <span class="n">data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dataset_fetcher</span><span class="o">.</span><span class="n">fetch</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>  <span class="c1"># may raise StopIteration</span>
<span class="g g-Whitespace">    </span><span class="mi">674</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">675</span>         <span class="n">data</span> <span class="o">=</span> <span class="n">_utils</span><span class="o">.</span><span class="n">pin_memory</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pin_memory_device</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\torch\utils\data\_utils\fetch.py:42,</span> in <span class="ni">_IterableDatasetFetcher.fetch</span><span class="nt">(self, possibly_batched_index)</span>
<span class="g g-Whitespace">     </span><span class="mi">40</span>         <span class="k">raise</span> <span class="ne">StopIteration</span>
<span class="g g-Whitespace">     </span><span class="mi">41</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">42</span>     <span class="n">data</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset_iter</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\load.py:140,</span> in <span class="ni">DataLoader.create_batches</span><span class="nt">(self, samps)</span>
<span class="g g-Whitespace">    </span><span class="mi">138</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">139</span> <span class="n">res</span> <span class="o">=</span> <span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span><span class="n">o</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">do_item</span><span class="p">,</span> <span class="n">samps</span><span class="p">))</span>
<span class="ne">--&gt; </span><span class="mi">140</span> <span class="k">yield from</span> <span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">do_batch</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">chunkify</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\basics.py:253,</span> in <span class="ni">chunked</span><span class="nt">(it, chunk_sz, drop_last, n_chunks)</span>
<span class="g g-Whitespace">    </span><span class="mi">251</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">Iterator</span><span class="p">):</span> <span class="n">it</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">it</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span> <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">253</span>     <span class="n">res</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="n">chunk_sz</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>     <span class="k">if</span> <span class="n">res</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">==</span><span class="n">chunk_sz</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">drop_last</span><span class="p">):</span> <span class="k">yield</span> <span class="n">res</span>
<span class="g g-Whitespace">    </span><span class="mi">255</span>     <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span><span class="o">&lt;</span><span class="n">chunk_sz</span><span class="p">:</span> <span class="k">return</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\load.py:170,</span> in <span class="ni">DataLoader.do_item</span><span class="nt">(self, s)</span>
<span class="g g-Whitespace">    </span><span class="mi">169</span> <span class="k">def</span> <span class="nf">do_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">170</span>     <span class="k">try</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">after_item</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">create_item</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">171</span>     <span class="k">except</span> <span class="n">SkipItemException</span><span class="p">:</span> <span class="k">return</span> <span class="kc">None</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\load.py:177,</span> in <span class="ni">DataLoader.create_item</span><span class="nt">(self, s)</span>
<span class="g g-Whitespace">    </span><span class="mi">176</span> <span class="k">def</span> <span class="nf">create_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">177</span>     <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">indexed</span><span class="p">:</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">s</span> <span class="ow">or</span> <span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">178</span>     <span class="k">elif</span> <span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>  <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">it</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">179</span>     <span class="k">else</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">IndexError</span><span class="p">(</span><span class="s2">&quot;Cannot index an iterable dataset numerically - must use `None`.&quot;</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\core.py:449,</span> in <span class="ni">Datasets.__getitem__</span><span class="nt">(self, it)</span>
<span class="g g-Whitespace">    </span><span class="mi">448</span> <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">it</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">449</span>     <span class="n">res</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">tl</span><span class="p">[</span><span class="n">it</span><span class="p">]</span> <span class="k">for</span> <span class="n">tl</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tls</span><span class="p">])</span>
<span class="g g-Whitespace">    </span><span class="mi">450</span>     <span class="k">return</span> <span class="n">res</span> <span class="k">if</span> <span class="n">is_indexer</span><span class="p">(</span><span class="n">it</span><span class="p">)</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">res</span><span class="p">))</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\core.py:408,</span> in <span class="ni">TfmdLists.__getitem__</span><span class="nt">(self, idx)</span>
<span class="g g-Whitespace">    </span><span class="mi">406</span> <span class="n">res</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">407</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_after_item</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="n">res</span>
<span class="ne">--&gt; </span><span class="mi">408</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_after_item</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_indexer</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">else</span> <span class="n">res</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_after_item</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\data\core.py:368,</span> in <span class="ni">TfmdLists._after_item</span><span class="nt">(self, o)</span>
<span class="g g-Whitespace">    </span><span class="mi">366</span>         <span class="k">raise</span>
<span class="g g-Whitespace">    </span><span class="mi">367</span> <span class="k">def</span> <span class="nf">subset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_new</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">split_idx</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">368</span> <span class="k">def</span> <span class="nf">_after_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tfms</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">369</span> <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="si">}</span><span class="se">\n</span><span class="s2">tfms - </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">tfms</span><span class="o">.</span><span class="n">fs</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="nn">    370 def __iter__(self): return (self[i] for i</span> in <span class="ni">range(len</span><span class="nt">(self)))</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\transform.py:210,</span> in <span class="ni">Pipeline.__call__</span><span class="nt">(self, o)</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span>     <span class="bp">self</span><span class="o">.</span><span class="n">fs</span><span class="o">+=</span><span class="n">ts</span>
<span class="g g-Whitespace">    </span><span class="mi">208</span>     <span class="bp">self</span><span class="o">.</span><span class="n">fs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">sorted</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s1">&#39;order&#39;</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">210</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">o</span><span class="p">):</span> <span class="k">return</span> <span class="n">compose_tfms</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">tfms</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fs</span><span class="p">,</span> <span class="n">split_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_idx</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">211</span> <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Pipeline: </span><span class="si">{</span><span class="s1">&#39; -&gt; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">f</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">fs</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">f</span><span class="o">.</span><span class="n">name</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s1">&#39;noop&#39;</span><span class="p">])</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">212</span> <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">i</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\transform.py:160,</span> in <span class="ni">compose_tfms</span><span class="nt">(x, tfms, is_enc, reverse, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">158</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">tfms</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">159</span>     <span class="k">if</span> <span class="ow">not</span> <span class="n">is_enc</span><span class="p">:</span> <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">decode</span>
<span class="ne">--&gt; </span><span class="mi">160</span>     <span class="n">x</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">161</span> <span class="k">return</span> <span class="n">x</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\transform.py:83,</span> in <span class="ni">Transform.__call__</span><span class="nt">(self, x, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">81</span> <span class="nd">@property</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span> <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_name&#39;</span><span class="p">,</span> <span class="n">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>
<span class="ne">---&gt; </span><span class="mi">83</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="s1">&#39;encodes&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">84</span> <span class="k">def</span> <span class="nf">decode</span>  <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call</span><span class="p">(</span><span class="s1">&#39;decodes&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span> <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s1">:</span><span class="se">\n</span><span class="s1">encodes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">encodes</span><span class="si">}</span><span class="s1">decodes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">decodes</span><span class="si">}</span><span class="s1">&#39;</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\transform.py:93,</span> in <span class="ni">Transform._call</span><span class="nt">(self, fn, x, split_idx, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">91</span> <span class="k">def</span> <span class="nf">_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">split_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">92</span>     <span class="k">if</span> <span class="n">split_idx</span><span class="o">!=</span><span class="bp">self</span><span class="o">.</span><span class="n">split_idx</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="n">x</span>
<span class="ne">---&gt; </span><span class="mi">93</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_call</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\transform.py:99,</span> in <span class="ni">Transform._do_call</span><span class="nt">(self, f, x, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">97</span>     <span class="k">if</span> <span class="n">f</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="n">x</span>
<span class="g g-Whitespace">     </span><span class="mi">98</span>     <span class="n">ret</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">returns</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="s1">&#39;returns&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="kc">None</span>
<span class="ne">---&gt; </span><span class="mi">99</span>     <span class="k">return</span> <span class="n">retain_type</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">ret</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">100</span> <span class="n">res</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_do_call</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span> <span class="k">return</span> <span class="n">retain_type</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastcore\dispatch.py:122,</span> in <span class="ni">TypeDispatch.__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">120</span> <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">inst</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">f</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inst</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">121</span> <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">owner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">f</span> <span class="o">=</span> <span class="n">MethodType</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">owner</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">122</span> <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\vision\core.py:127,</span> in <span class="ni">PILBase.create</span><span class="nt">(cls, fn, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">125</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span><span class="nb">bytes</span><span class="p">):</span> <span class="n">fn</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">126</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span><span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">):</span> <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">127</span> <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">load_image</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="o">**</span><span class="n">merge</span><span class="p">(</span><span class="bp">cls</span><span class="o">.</span><span class="n">_open_args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)))</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\fastai\vision\core.py:100,</span> in <span class="ni">load_image</span><span class="nt">(fn, mode)</span>
<span class="g g-Whitespace">     </span><span class="mi">98</span> <span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">     </span><span class="mi">99</span>     <span class="s2">&quot;Open and load a `PIL.Image` and convert to `mode`&quot;</span>
<span class="ne">--&gt; </span><span class="mi">100</span>     <span class="n">im</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">101</span>     <span class="n">im</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
<span class="g g-Whitespace">    </span><span class="mi">102</span>     <span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">_new</span><span class="p">(</span><span class="n">im</span><span class="o">.</span><span class="n">im</span><span class="p">)</span>

<span class="nn">File ~\anaconda3\envs\book\Lib\site-packages\PIL\Image.py:3431,</span> in <span class="ni">open</span><span class="nt">(fp, mode, formats)</span>
<span class="g g-Whitespace">   </span><span class="mi">3428</span>     <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">fspath</span><span class="p">(</span><span class="n">fp</span><span class="p">))</span>
<span class="g g-Whitespace">   </span><span class="mi">3430</span> <span class="k">if</span> <span class="n">filename</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">3431</span>     <span class="n">fp</span> <span class="o">=</span> <span class="n">builtins</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">3432</span>     <span class="n">exclusive_fp</span> <span class="o">=</span> <span class="kc">True</span>
<span class="g g-Whitespace">   </span><span class="mi">3433</span> <span class="k">else</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>The automatic calculation of gradients profoundly simplifies the implementation of deep learning algorithms. Before automatic differentiation, even small changes to complicated models required recalculating complicated derivatives by hand. Surprisingly often, academic papers had to allocate numerous pages to deriving update rules. While we must continue to rely on automatic differentiation so we can focus on the interesting parts, you ought to know how these gradients are calculated under the hood if you want to go beyond a shallow understanding of deep learning.</p>
<section id="forward-propagation">
<h3>Forward Propagation<a class="headerlink" href="#forward-propagation" title="Link to this heading">#</a></h3>
<p><em>Forward propagation</em> (or <em>forward pass</em>) is the process of computing intermediate variables in a neural network from the input layer to the output layer.</p>
<p>Given an input <span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^d\)</span>, the hidden layer (without bias) first computes</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} = \mathbf{W}^{(1)} \mathbf{x}, \quad \mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}.
\]</div>
<p>Applying the activation function <span class="math notranslate nohighlight">\(\phi\)</span> gives the hidden representation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{h} = \phi(\mathbf{z}), \quad \mathbf{h}\in\mathbb{R}^h.
\]</div>
<p>The output layer then produces</p>
<div class="math notranslate nohighlight">
\[
\mathbf{o} = \mathbf{W}^{(2)} \mathbf{h}, \quad \mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}.
\]</div>
<p>With a loss function <span class="math notranslate nohighlight">\(l\)</span> and label <span class="math notranslate nohighlight">\(y\)</span>, the loss for a single example is</p>
<div class="math notranslate nohighlight">
\[
L = l(\mathbf{o}, y).
\]</div>
<p>Including <span class="math notranslate nohighlight">\(\ell\_2\)</span> regularization with hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span>, the penalty term is</p>
<div class="math notranslate nohighlight">
\[
s = \tfrac{\lambda}{2}\left(\|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(|\cdot|_F\)</span> denotes the Frobenius norm.</p>
<p>Finally, the regularized objective function is</p>
<div class="math notranslate nohighlight">
\[
J = L + s.
\]</div>
</section>
<section id="backpropagation">
<h3>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">#</a></h3>
<p><em>Backpropagation</em> is the method for computing gradients of neural network parameters by traversing the network in reverse, from output to input, using the chain rule of calculus. Intermediate partial derivatives are stored during the process.</p>
<p>For functions <span class="math notranslate nohighlight">\(\mathsf{Y}=f(\mathsf{X})\)</span> and <span class="math notranslate nohighlight">\(\mathsf{Z}=g(\mathsf{Y})\)</span>, the chain rule gives</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \text{prod}\!\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{prod}\)</span> denotes the correct multiplication (matrix–matrix or tensor equivalent).</p>
<p>In our network with parameters <span class="math notranslate nohighlight">\(\mathbf{W}^{(1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{W}^{(2)}\)</span>, the objective is <span class="math notranslate nohighlight">\(J=L+s\)</span>, where <span class="math notranslate nohighlight">\(L\)</span> is the loss and <span class="math notranslate nohighlight">\(s\)</span> the <span class="math notranslate nohighlight">\(\ell\_2\)</span> regularization term. The first gradients are straightforward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial L}=1, \qquad \frac{\partial J}{\partial s}=1.
\]</div>
<p>From here, we work backwards. The gradient with respect to the output layer variable is</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{o}} \in \mathbb{R}^q.
\]</div>
<p>The regularization gradients are</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)}, \qquad 
\frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}.
\]</div>
<p>Thus, for the output layer parameters we obtain</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial \mathbf{W}^{(2)}} = \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}.
\]</div>
<p>Moving to the hidden layer, the gradients are propagated as</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial \mathbf{h}} = {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}, 
\]</div>
<p>and, applying the elementwise derivative of the activation function,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial \mathbf{h}} \odot \phi'(\mathbf{z}).
\]</div>
<p>Finally, for the hidden layer parameters,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial \mathbf{W}^{(1)}} = \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}.
\]</div>
</section>
<section id="the-cross-entropy-loss">
<h3>The Cross-Entropy Loss<a class="headerlink" href="#the-cross-entropy-loss" title="Link to this heading">#</a></h3>
<p>In multi-class classification, the commonly used loss function is the <em>cross-entropy loss</em>, also known as log loss.</p>
<p>This choice can be easily justified with maximum likelihood estimation.</p>
<p>The softmax function gives us a vector <span class="math notranslate nohighlight">\(\mathbf{p}\)</span>, which
we can interpret as the (estimated) conditional probabilities of each
class, given any input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, such as <span class="math notranslate nohighlight">\(p_1\)</span> =
<span class="math notranslate nohighlight">\(P(y=\textrm{cat} \mid \mathbf{x})\)</span>.
In the following we assume that for a dataset with features <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> the labels
<span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> are represented using a one-hot encoding label
vector. We can compare the estimates with reality by checking how
probable the actual classes are according to our model, given the
features:</p>
<div class="math notranslate nohighlight">
\[   P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}). \]</div>
<p>We are allowed to use the factorization since we assume that each label
is drawn independently from its respective distribution
<span class="math notranslate nohighlight">\(P(\mathbf{y}\mid\mathbf{x}^{(i)})\)</span>. Then, we take the negative logarithm to obtain the
equivalent problem of minimizing the negative log-likelihood:</p>
<div class="math notranslate nohighlight">
\[   -\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
   = \sum_{i=1}^n l(\mathbf{y}^{(i)}, \mathbf{p}^{(i)}),\]</div>
<p>where for any pair of label <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and model prediction
<span class="math notranslate nohighlight">\(\mathbf{p}\)</span> over <span class="math notranslate nohighlight">\(C\)</span> classes, the loss function
<span class="math notranslate nohighlight">\(L\)</span> is</p>
<div class="math notranslate nohighlight">
\[  L(\mathbf{y}, \mathbf{p}) = - \sum_{j=1}^C y_j \log \hat{y}_j. \]</div>
<p>This equation is commonly called the <em>cross-entropy
loss</em>. Since <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a one-hot vector of length <span class="math notranslate nohighlight">\(C\)</span>,
the sum over all its coordinates <span class="math notranslate nohighlight">\(j\)</span> vanishes for all but one
term. Note that the loss <span class="math notranslate nohighlight">\(L(\mathbf{y}, \mathbf{p})\)</span> is
bounded from below by <span class="math notranslate nohighlight">\(0\)</span> whenever <span class="math notranslate nohighlight">\(\mathbf{p}\)</span> is a
probability vector: no single entry is larger than <span class="math notranslate nohighlight">\(1\)</span>, hence
their negative logarithm cannot be lower than <span class="math notranslate nohighlight">\(0\)</span>;
<span class="math notranslate nohighlight">\(L(\mathbf{y}, \mathbf{p}) = 0\)</span> only if we predict the
actual label with <em>certainty</em>. This can never happen for any finite
setting of the weights because taking a softmax output towards <span class="math notranslate nohighlight">\(1\)</span>
requires taking the corresponding input <span class="math notranslate nohighlight">\(o_i\)</span> to infinity (or all
other outputs <span class="math notranslate nohighlight">\(o_j\)</span> for <span class="math notranslate nohighlight">\(j \neq i\)</span> to negative infinity).
Even if our model could assign an output probability of <span class="math notranslate nohighlight">\(0\)</span>, any
error made when assigning such high confidence would incur infinite loss
(<span class="math notranslate nohighlight">\(-\log 0 = \infty\)</span>).</p>
</section>
</section>
<section id="interpreting-the-model">
<h2>Interpreting the Model<a class="headerlink" href="#interpreting-the-model" title="Link to this heading">#</a></h2>
<p>After training, it’s helpful to interpret the model’s performance to understand areas of improvement. A confusion matrix is useful for identifying patterns in misclassifications.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interp</span> <span class="o">=</span> <span class="n">ClassificationInterpretation</span><span class="o">.</span><span class="n">from_learner</span><span class="p">(</span><span class="n">learn</span><span class="p">)</span>
<span class="n">interp</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"></div><div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"></div><img alt="../_images/3db47d89b3ebd52691d700566177aec4d3c03ec336ae2b48f8877c451c7f2086.png" src="../_images/3db47d89b3ebd52691d700566177aec4d3c03ec336ae2b48f8877c451c7f2086.png" />
</div>
</div>
<p>When dealing with a large number of classes, the confusion matrix can become complex. The <code class="docutils literal notranslate"><span class="pre">most_confused</span></code> method highlights the most common errors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interp</span><span class="o">.</span><span class="n">most_confused</span><span class="p">(</span><span class="n">min_val</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"></div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;5&#39;, &#39;3&#39;, 52),
 (&#39;6&#39;, &#39;2&#39;, 49),
 (&#39;3&#39;, &#39;5&#39;, 45),
 (&#39;9&#39;, &#39;7&#39;, 41),
 (&#39;4&#39;, &#39;9&#39;, 32),
 (&#39;8&#39;, &#39;3&#39;, 27)]
</pre></div>
</div>
</div>
</div>
<p>This output shows which breeds the model often confuses, providing insights into areas needing improvement.</p>
</section>
<section id="improving-the-model">
<h2>Improving the Model<a class="headerlink" href="#improving-the-model" title="Link to this heading">#</a></h2>
<section id="the-learning-rate">
<h3>The Learning Rate<a class="headerlink" href="#the-learning-rate" title="Link to this heading">#</a></h3>
<p>The learning rate <span class="math notranslate nohighlight">\( \eta \)</span> controls the step size in the gradient descent optimization process. At each training step, the model updates its weights <span class="math notranslate nohighlight">\( w \)</span> in the opposite direction of the gradient <span class="math notranslate nohighlight">\( \nabla L(w) \)</span> of the loss function <span class="math notranslate nohighlight">\( L \)</span> with respect to the weights. This update rule is:</p>
<div class="math notranslate nohighlight">
\[
w_{i+1} = w_i - \eta \nabla L(w_i)
\]</div>
<p>If <span class="math notranslate nohighlight">\( \eta \)</span> is too large, the model may overshoot the optimal weights, causing unstable training. If <span class="math notranslate nohighlight">\( \eta \)</span> is too small, training may take too long to converge. The optimal learning rate is usually found by experimenting or using a learning rate finder.</p>
</section>
<section id="learning-rate-finder">
<h3>Learning Rate Finder<a class="headerlink" href="#learning-rate-finder" title="Link to this heading">#</a></h3>
<p>The learning rate finder in fastai allows us to identify a suitable learning rate by gradually increasing it and observing the effect on the loss. Here’s how to apply it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_min</span><span class="p">,</span> <span class="n">lr_steep</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">(</span><span class="n">suggest_funcs</span><span class="o">=</span><span class="p">(</span><span class="n">minimum</span><span class="p">,</span> <span class="n">steep</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Minimum/10: </span><span class="si">{</span><span class="n">lr_min</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">, steepest point: </span><span class="si">{</span><span class="n">lr_steep</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<style>
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    progress:not([value]), progress:not([value])::-webkit-progress-bar {
        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
</style>
</div><div class="output text_html"></div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\biagi\anaconda3\envs\book\Lib\site-packages\fastai\learner.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don&#39;t have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(file, map_location=device, **torch_load_kwargs)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minimum/10: 1.32e-07, steepest point: 1.10e-06
</pre></div>
</div>
<img alt="../_images/d68435147c5ce551bc3f9b98fd98405117455c2c51b0dd81b8a969c2c06ab1eb.png" src="../_images/d68435147c5ce551bc3f9b98fd98405117455c2c51b0dd81b8a969c2c06ab1eb.png" />
</div>
</div>
<blockquote>
<div><p>Note: Logarithmic Scale: The learning rate finder plot has a logarithmic scale, which is why the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3. This is because we care mostly about the order of magnitude of the learning rate.</p>
</div></blockquote>
<p>The learning rate plot helps identify a rate that minimizes loss without diverging. Choose a rate slightly lower than the minimum or steepest point, around 3e-3 in this case.</p>
</section>
<section id="early-stopping">
<h3>Early Stopping<a class="headerlink" href="#early-stopping" title="Link to this heading">#</a></h3>
<p>Deep neural networks can eventually memorize even random labels (Zhang et al., 2021). However, research shows they fit clean labels first and only later adapt to mislabeled ones (Rolnick et al., 2017). This leads to an important insight: if a model fits the correct labels but not the noisy ones, it has generalized (Garg et al., 2021).</p>
<p>This motivates <strong>early stopping</strong>, a classic regularization method. Instead of limiting weights directly, we limit the training duration. The usual practice is to monitor validation error at each epoch and stop when it fails to improve beyond a small threshold for a set number of epochs, often called the <em>patience criterion</em>.</p>
<p>Early stopping offers two advantages:</p>
<ol class="arabic simple">
<li><p>Better generalization in the presence of noisy labels.</p></li>
<li><p>Reduced training cost and time, especially for large models requiring multi-GPU setups.</p></li>
</ol>
<p>When data is noise-free and fully separable (e.g., cats vs. dogs), early stopping rarely improves generalization. But in cases with label noise or inherent uncertainty (e.g., predicting patient outcomes), stopping before the model memorizes noise is essential.</p>
</section>
<section id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h3>
<p>A good predictive model should generalize well to unseen data. Classical theory links generalization to <em>simplicity</em>, which can mean low dimensionality, small parameter norms (as in weight decay), or smoothness—insensitivity to small perturbations in the input. For example, image classifiers should be robust to minor pixel noise.</p>
<p>Bishop (1995) showed that training with input noise is equivalent to Tikhonov regularization, formalizing the link between smoothness and robustness. Building on this idea, Srivastava et al. (2014) proposed <strong>dropout</strong>, a technique that applies noise to the <em>internal layers</em> of neural networks. During training, dropout randomly “drops” (zeros out) a fraction of neurons at each iteration, preventing layers from relying too heavily on specific activation patterns—a problem they call <em>co-adaptation</em>.</p>
<p>The mechanism is simple: with dropout probability <span class="math notranslate nohighlight">\(p\)</span>, each activation <span class="math notranslate nohighlight">\(h\)</span> is replaced by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{h} = \begin{cases} 
0 &amp; \text{with prob. } p, \\ 
\frac{h}{1-p} &amp; \text{with prob. } 1-p,
\end{cases}
\end{split}\]</div>
<p>so that the expectation <span class="math notranslate nohighlight">\( \mathbb{E}[\tilde{h}] = h \)</span> remains unchanged.</p>
<p>Typically, dropout is diabled at test time. Given a trained model and a new example, we do not drop out any nodes and thus do not need to normalize. However, there are some exceptions: some researchers use dropout at test time as a heuristic for estimating the uncertainty of neural network predictions: if the predictions agree across many different dropout outputs, then we might say that the network is more confident.</p>
<p>Dropout has become a standard regularization method in deep learning, helping models avoid overfitting and improving generalization, especially when training large networks.</p>
</section>
<section id="further-optimization-selecting-epochs-and-avoiding-overfitting">
<h3>Further Optimization: Selecting Epochs and Avoiding Overfitting<a class="headerlink" href="#further-optimization-selecting-epochs-and-avoiding-overfitting" title="Link to this heading">#</a></h3>
<p>Choosing the number of epochs is a balancing act. Too few epochs can result in underfitting, while too many can lead to overfitting. Here are some general guidelines:</p>
<ul class="simple">
<li><p>Start with a few epochs to see baseline performance.</p></li>
<li><p>Monitor validation loss and accuracy over epochs to determine when the model starts overfitting.</p></li>
<li><p>Use early stopping or cross-validation if needed.</p></li>
</ul>
<p>If training resources allow, try experimenting with larger models or deeper architectures. However, always validate if additional complexity improves your specific task.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="23_Loss_Metrics_and_Optimizers.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Training a Digit Classifier</p>
      </div>
    </a>
    <a class="right-next"
       href="25_convolutions.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Convolutional Neural Networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-multilayer-perceptrons">Implementation of Multilayer Perceptrons</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-model-parameters">Initializing Model Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-datablock">Building the DataBlock</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-datablock-parameters">Explanation of the <code class="docutils literal notranslate"><span class="pre">DataBlock</span></code> Parameters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validating-the-data-pipeline">Validating the Data Pipeline</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model">Training the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation">Forward Propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cross-entropy-loss">The Cross-Entropy Loss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-the-model">Interpreting the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improving-the-model">Improving the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate">The Learning Rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-finder">Learning Rate Finder</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping">Early Stopping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#further-optimization-selecting-epochs-and-avoiding-overfitting">Further Optimization: Selecting Epochs and Avoiding Overfitting</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Biagio Mandracchia
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>